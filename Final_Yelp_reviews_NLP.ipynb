{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Natural Language Processing on the Yelp reviews dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in some Yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split,StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the reviews into a DataFrame\n",
    "yelp = pd.read_csv(\"yelp_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5    432637\n",
       "4    229402\n",
       "1    140308\n",
       "3    114052\n",
       "2     83601\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(yelp.shape)\n",
    "#Looking into the dataset\n",
    "yelp.head()\n",
    "\n",
    "#Value counts of 'stars'\n",
    "\n",
    "yelp['stars'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Figuring out the number of reviews that businesses receive\n",
    "number_of_reviews = yelp.business_id.value_counts().value_counts().plot()\n",
    "sns.mpl.pyplot.xlabel(\"number of reviews/business\")\n",
    "sns.mpl.pyplot.ylabel(\"number of businesses\")\n",
    "sns.mpl.pyplot.xlim(0,1000)\n",
    "sns.mpl.pyplot.ylim(0,1000)\n",
    "sns.mpl.pyplot.show()\n",
    "\n",
    "#Most businesses have very few reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#reviews per user\n",
    "yelp.user_id.value_counts().value_counts().sort_index().plot()\n",
    "sns.mpl.pyplot.xlabel(\"number of reviews/user\")\n",
    "sns.mpl.pyplot.ylabel(\"number of users\")\n",
    "sns.mpl.pyplot.xlim(0,1000)\n",
    "sns.mpl.pyplot.ylim(0,1000)\n",
    "sns.mpl.pyplot.show()\n",
    "\n",
    "#Both these graphs seem to denote a power law relationship. I plot a log-log plot as the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#reviews per business\n",
    "yelp.business_id.value_counts().value_counts().sort_index().plot(loglog=True)\n",
    "sns.mpl.pyplot.xlabel(\"number of reviews/business\")\n",
    "sns.mpl.pyplot.ylabel(\"number of businesses\")\n",
    "sns.mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#reviews per user\n",
    "yelp.user_id.value_counts().value_counts().sort_index().plot(loglog=True)\n",
    "sns.mpl.pyplot.xlabel(\"number of reviews/user\")\n",
    "sns.mpl.pyplot.ylabel(\"number of users\")\n",
    "sns.mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To retain meaningful data, we can only look at businesses with > 10 reviews and users with > 10 reviews\n",
    "#Converting to DataFrame\n",
    "yelp = pd.DataFrame(yelp)\n",
    "\n",
    "yelp_smaller = yelp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    95129\n",
      "0    18562\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aov96CM4FZAXeZvKtsStdA</td>\n",
       "      <td>5</td>\n",
       "      <td>This place is awesome! Definitely authentic!!!...</td>\n",
       "      <td>u0LXt3Uea_GidxRW1xcsfg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zgQHtqX0gqMw1nlBZl2VnQ</td>\n",
       "      <td>1</td>\n",
       "      <td>really excited to hear of this restaurant comi...</td>\n",
       "      <td>u0LXt3Uea_GidxRW1xcsfg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hjk3ox7w1akbEuOgTJ03Bw</td>\n",
       "      <td>1</td>\n",
       "      <td>Food is very bland - not authentic at all.\\n\\n...</td>\n",
       "      <td>u0LXt3Uea_GidxRW1xcsfg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tOhRQqiupLyJdBJVQMGOEQ</td>\n",
       "      <td>5</td>\n",
       "      <td>OMG - Definitely worth going if you are in Mon...</td>\n",
       "      <td>u0LXt3Uea_GidxRW1xcsfg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I8rveLd-dl81u6c8YqAxmw</td>\n",
       "      <td>1</td>\n",
       "      <td>If you have not yet tried Wasabi - don't bothe...</td>\n",
       "      <td>u0LXt3Uea_GidxRW1xcsfg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  stars  \\\n",
       "0  Aov96CM4FZAXeZvKtsStdA      5   \n",
       "1  zgQHtqX0gqMw1nlBZl2VnQ      1   \n",
       "2  hjk3ox7w1akbEuOgTJ03Bw      1   \n",
       "3  tOhRQqiupLyJdBJVQMGOEQ      5   \n",
       "4  I8rveLd-dl81u6c8YqAxmw      1   \n",
       "\n",
       "                                                text                 user_id  \\\n",
       "0  This place is awesome! Definitely authentic!!!...  u0LXt3Uea_GidxRW1xcsfg   \n",
       "1  really excited to hear of this restaurant comi...  u0LXt3Uea_GidxRW1xcsfg   \n",
       "2  Food is very bland - not authentic at all.\\n\\n...  u0LXt3Uea_GidxRW1xcsfg   \n",
       "3  OMG - Definitely worth going if you are in Mon...  u0LXt3Uea_GidxRW1xcsfg   \n",
       "4  If you have not yet tried Wasabi - don't bothe...  u0LXt3Uea_GidxRW1xcsfg   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       0  \n",
       "2       0  \n",
       "3       1  \n",
       "4       0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keep only businesses with > 10 reviews, users with > 10 reviews\n",
    "yelp_smaller = yelp.groupby(\"business_id\").filter(lambda x: x.shape[0]>10)\n",
    "yelp_smaller = yelp_smaller.groupby(\"user_id\").filter(lambda x: x.shape[0]>10)\n",
    "#and only the 5-star and <=1 star reviews\n",
    "yelp_smaller = yelp_smaller[(yelp_smaller.stars==5) | (yelp_smaller.stars==1)]\n",
    "#keep only columns we would want to use later\n",
    "\n",
    "yelp_smaller[\"target\"] = (yelp_smaller.stars >1).astype(int)\n",
    "print(yelp_smaller.target.value_counts())\n",
    "\n",
    "yelp_smaller.reset_index(drop=True,inplace=True)\n",
    "\n",
    "yelp_smaller.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **corpus:** collection of documents\n",
    "- **corpora:** plural form of corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using StratifiedShuffleSplit to preserve imbalance in the target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the new DataFrame into training and testing sets, keeping relative frequencies of targets unchanged\n",
    "splitter = StratifiedShuffleSplit(n_splits=2, test_size=0.4,random_state=1)\n",
    "train_indices,test_indices=list(splitter.split(yelp_smaller.text,yelp_smaller.target))[0]\n",
    "X_train,y_train = yelp_smaller.text.iloc[train_indices],yelp_smaller.target.iloc[train_indices]\n",
    "X_test,y_test = yelp_smaller.text.iloc[test_indices],yelp_smaller.target.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.873128\n",
       "0    0.126872\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()/y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.720749\n",
       "0    0.279251\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-edba2148e1ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# use CountVectorizer to create document-term matrices from X_train and X_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_dtm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtest_dtm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    801\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 803\u001b[1;33m             \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    804\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m             \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use CountVectorizer to create document-term matrices from X_train and X_test\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "train_dtm = vect.fit_transform(X_train)\n",
    "test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99158, 109198)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows are documents, columns are features\n",
    "train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['05a', '05am', '05p', '05pm', '06', '0600', '0601', '0630', '06pm', '07', '0700', '0716', '0730', '074', '0745hrs', '0757', '0777', '079', '07pm', '08', '0800hrs', '0830', '0835', '0840', '0845', '08am', '09', '090', '0900', '0905', '0908', '091', '0912', '094', '09am', '09pm', '0OqUYIsQ', '0P', '0_0', '0admission', '0f', '0h', '0ur', '0ut', '0utta', '0z', '10', '100', '1000', '10000', '100000', '1000000', '100000000000', '100000000000000000000000000000000000000000000', '10000000X', '10000degrees', '1000X', '1000npm', '1000s', '1000x', '1001', '100111fa_fact_seabrook', '1003rd', '1008', '100F', '100Fs', '100K', '100ft', '100g', '100ish', '100k', '100km', '100lbs', '100mbps', '100mg', '100ml', '100mm', '100oz', '100percent', '100pp', '100s', '100th', '100x', '101', '1012', '10127', '1015', '10159531', '101st', '102', '1023', '1024GB', '1026', '1029', '103', '1030', '1030am', '1030ish', '1030pm', '10345', '103rd', '104', '1040', '10420', '1045', '1045a', '1045pm', '105', '1050', '1055', '1057', '1058', '105degree', '105each', '105o', '106', '106k', '106th', '107', '107th', '108', '1080P', '1082', '108th', '109', '1092nd', '1097967', '109th', '10AM', '10AWG', '10Am', '10DEeac', '10K', '10P', '10PM', '10X', '10a', '10am', '10az', '10bucks', '10days', '10different', '10ea', '10g', '10h', '10inch', '10inites', '10ish', '10k', '10lb', '10lbs', '10m', '10mbps', '10miles', '10min', '10mins', '10minutes', '10months', '10more', '10oz', '10p', '10pc', '10pm', '10ppl', '10s', '10th', '10ths', '10x', '10x15', '10yards', '10years', '10yo', '10yr', '10yrs', '11', '110', '1100', '1101794', '1102', '110F', '111', '1111111', '1113', '1115', '111am', '112', '113', '1130', '1130am', '1130ish', '1130pm', '114', '1140', '1142', '1145', '1146', '114pound', '115', '115020347051893738999', '1155PM', '1158', '1159', '115F', '116', '117', '117th', '118', '119', '1195', '11AM', '11PM', '11PMish', '11a', '11am', '11h', '11in', '11ish', '11min', '11months', '11mos', '11oz', '11p', '11pm', '11s', '11th', '11yrs', '12', '120', '1200', '12000', '1200F', '1200sq', '1207PM', '120K', '120MPH', '120degree', '120k', '120lb', '120min', '120mph', '120pm', '121', '1213', '1215', '122', '1220', '1226', '123', '1230', '1230am', '1232', '1234', '1236', '123k', '124', '1242pm', '125', '1250am', '125ml', '126', '1260626', '127', '1275', '128', '128i', '129', '129286', '12AM', '12K', '12PM', '12a', '12am', '12ft', '12h', '12in', '12ish', '12k', '12min', '12months', '12noon', '12oz', '12p', '12pc', '12pcs', '12pm', '12th', '12x', '12yr', '12yrs', '13', '130', '1300', '130am', '130k', '130pm', '131', '131f', '132', '13210247', '133', '1330', '133354', '134', '134th', '135', '135k', '137', '1377384893', '137f', '138', '1380', '138oz', '139', '1397', '139776', '13BvQFe', '13g', '13miles', '13oz', '13pp', '13th', '13yo', '13yr', '14', '140', '1400', '14000', '1400s', '140k', '141', '1418', '142', '143', '1430', '1432', '144', '1445', '145', '1450', '1455', '146', '147', '148', '1483', '149', '14998', '14Angostura', '14in', '14k', '14mile', '14oz', '14pm', '14th', '14years', '14yo', '15', '150', '1500', '1500hrs', '150mbps', '150plus', '150pp', '150th', '151', '1511', '152', '1525', '153', '154', '1540', '155', '1554', '1557', '158', '159', '1595', '15AM', '15K', '15MB', '15PM', '15Pm', '15Roasted', '15W', '15X', '15a', '15aine', '15am', '15ave', '15c', '15cents', '15ft', '15ish', '15k', '15km', '15m', '15min', '15mins', '15minute', '15minutes', '15ml', '15oz', '15p', '15pm', '15ppl', '15ppm', '15serves', '15th', '15x', '15yrs', '16', '160', '1600', '1600s', '1607', '161', '1616', '162', '163', '1630', '164', '165', '166', '1664', '167', '1670', '168', '168s', '169', '16CAD', '16F', '16Goat', '16Halloumi', '16g', '16gb', '16in', '16inch', '16lbs', '16ox', '16oz', '16pm', '16st', '16th', '16yo', '16yr', '17', '170', '1700', '1701D', '1705', '170K']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print(vect.get_feature_names()[50:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not change the text to lowercase because uppercase reviews might actually convey meaning in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99158, 109282)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# allow tokens of one character\n",
    "vect = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b', lowercase= False)\n",
    "train_dtm = vect.fit_transform(X_train)\n",
    "test_dtm = vect.fit_transform(X_test)\n",
    "train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yikes', 'yo', 'yogurt', 'yoke', 'yolk', 'yolks', 'york', 'yorkies', 'you', 'young', 'younger', 'youngest', 'your', 'youre', 'yours', 'yourself', 'yourselves', 'youtube', 'yr', 'yrs', 'yuck', 'yucky', 'yum', 'yumminess', 'yummy', 'yup', 'yuppies', 'yuppy', 'zactly', 'zealous', 'zero', 'ziemliches', 'zinged', 'zinger', 'zip', 'ziplock', 'ziti', 'zocalo', 'zombie', 'zombies', 'zone', 'zoned', 'zoning', 'zoo', 'zoom', 'zucchini', 'zupas', 'zwar', 'área', 'überhaupt']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting 5(positive) vs. 1(negative) star rating:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8874133949191686\n"
     ]
    }
   ],
   "source": [
    "# # use default options for CountVectorizer\n",
    "# vect = CountVectorizer(lowercase = False)\n",
    "\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "train_dtm = vect.fit_transform(X_train)\n",
    "test_dtm = vect.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_dtm, y_train)\n",
    "y_pred_class = lr.predict(test_dtm)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate null accuracy\n",
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just using raw counts of different words gives us very good performance!\n",
    "\n",
    "Let's examine the most positive and most negative words (their coefficients will tell us whether they are most indicative of a positive or negative review)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coeffs = pd.DataFrame(list(zip(vect.get_feature_names(),lr.coef_[0])),columns=[\"word\",\"coeff\"])\n",
    "feature_coeffs = feature_coeffs.sort_values(by=\"coeff\",ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words most indicative of positive review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excellent</td>\n",
       "      <td>2.964825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>excellent</td>\n",
       "      <td>2.787336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Love</td>\n",
       "      <td>2.598844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazing</td>\n",
       "      <td>2.454324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>perfect</td>\n",
       "      <td>2.407325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word     coeff\n",
       "0  Excellent  2.964825\n",
       "1  excellent  2.787336\n",
       "2       Love  2.598844\n",
       "3    Amazing  2.454324\n",
       "4    perfect  2.407325"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words most indicative of negative review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109193</th>\n",
       "      <td>rude</td>\n",
       "      <td>-2.735447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109194</th>\n",
       "      <td>bland</td>\n",
       "      <td>-2.742936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109195</th>\n",
       "      <td>Worst</td>\n",
       "      <td>-2.912159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109196</th>\n",
       "      <td>poisoning</td>\n",
       "      <td>-3.400705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109197</th>\n",
       "      <td>worst</td>\n",
       "      <td>-3.440507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word     coeff\n",
       "109193       rude -2.735447\n",
       "109194      bland -2.742936\n",
       "109195      Worst -2.912159\n",
       "109196  poisoning -3.400705\n",
       "109197      worst -3.440507"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coeffs[\"abs_coeff\"] = feature_coeffs.coeff.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most predictive words, regardless of polarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "      <th>abs_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109197</th>\n",
       "      <td>worst</td>\n",
       "      <td>-3.440507</td>\n",
       "      <td>3.440507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109196</th>\n",
       "      <td>poisoning</td>\n",
       "      <td>-3.400705</td>\n",
       "      <td>3.400705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excellent</td>\n",
       "      <td>2.964825</td>\n",
       "      <td>2.964825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109195</th>\n",
       "      <td>Worst</td>\n",
       "      <td>-2.912159</td>\n",
       "      <td>2.912159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>excellent</td>\n",
       "      <td>2.787336</td>\n",
       "      <td>2.787336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109194</th>\n",
       "      <td>bland</td>\n",
       "      <td>-2.742936</td>\n",
       "      <td>2.742936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109193</th>\n",
       "      <td>rude</td>\n",
       "      <td>-2.735447</td>\n",
       "      <td>2.735447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Love</td>\n",
       "      <td>2.598844</td>\n",
       "      <td>2.598844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109192</th>\n",
       "      <td>disappointing</td>\n",
       "      <td>-2.565074</td>\n",
       "      <td>2.565074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109191</th>\n",
       "      <td>stale</td>\n",
       "      <td>-2.515708</td>\n",
       "      <td>2.515708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazing</td>\n",
       "      <td>2.454324</td>\n",
       "      <td>2.454324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109190</th>\n",
       "      <td>horrible</td>\n",
       "      <td>-2.426339</td>\n",
       "      <td>2.426339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>perfect</td>\n",
       "      <td>2.407325</td>\n",
       "      <td>2.407325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>delicious</td>\n",
       "      <td>2.395399</td>\n",
       "      <td>2.395399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109189</th>\n",
       "      <td>overpriced</td>\n",
       "      <td>-2.363638</td>\n",
       "      <td>2.363638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109188</th>\n",
       "      <td>tasteless</td>\n",
       "      <td>-2.344295</td>\n",
       "      <td>2.344295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109187</th>\n",
       "      <td>waste</td>\n",
       "      <td>-2.324308</td>\n",
       "      <td>2.324308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>awesome</td>\n",
       "      <td>2.296141</td>\n",
       "      <td>2.296141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109186</th>\n",
       "      <td>worse</td>\n",
       "      <td>-2.272379</td>\n",
       "      <td>2.272379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109185</th>\n",
       "      <td>mediocre</td>\n",
       "      <td>-2.260581</td>\n",
       "      <td>2.260581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109184</th>\n",
       "      <td>terrible</td>\n",
       "      <td>-2.244452</td>\n",
       "      <td>2.244452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109183</th>\n",
       "      <td>WORST</td>\n",
       "      <td>-2.240749</td>\n",
       "      <td>2.240749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Best</td>\n",
       "      <td>2.239415</td>\n",
       "      <td>2.239415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109182</th>\n",
       "      <td>sucks</td>\n",
       "      <td>-2.231289</td>\n",
       "      <td>2.231289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109181</th>\n",
       "      <td>unprofessional</td>\n",
       "      <td>-2.217887</td>\n",
       "      <td>2.217887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109180</th>\n",
       "      <td>disgusting</td>\n",
       "      <td>-2.171601</td>\n",
       "      <td>2.171601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Delicious</td>\n",
       "      <td>2.164957</td>\n",
       "      <td>2.164957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Awesome</td>\n",
       "      <td>2.154144</td>\n",
       "      <td>2.154144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Great</td>\n",
       "      <td>2.138344</td>\n",
       "      <td>2.138344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>fantastic</td>\n",
       "      <td>2.132356</td>\n",
       "      <td>2.132356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109153</th>\n",
       "      <td>inedible</td>\n",
       "      <td>-1.541175</td>\n",
       "      <td>1.541175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>definitely</td>\n",
       "      <td>1.539007</td>\n",
       "      <td>1.539007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109152</th>\n",
       "      <td>NOT</td>\n",
       "      <td>-1.537896</td>\n",
       "      <td>1.537896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>favorite</td>\n",
       "      <td>1.531097</td>\n",
       "      <td>1.531097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>disappoint</td>\n",
       "      <td>1.530840</td>\n",
       "      <td>1.530840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>flavorful</td>\n",
       "      <td>1.528547</td>\n",
       "      <td>1.528547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>fabulous</td>\n",
       "      <td>1.521572</td>\n",
       "      <td>1.521572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>outstanding</td>\n",
       "      <td>1.516476</td>\n",
       "      <td>1.516476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>exceptional</td>\n",
       "      <td>1.513732</td>\n",
       "      <td>1.513732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109151</th>\n",
       "      <td>slowest</td>\n",
       "      <td>-1.505931</td>\n",
       "      <td>1.505931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109150</th>\n",
       "      <td>rip</td>\n",
       "      <td>-1.503232</td>\n",
       "      <td>1.503232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>love</td>\n",
       "      <td>1.501592</td>\n",
       "      <td>1.501592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109149</th>\n",
       "      <td>dirty</td>\n",
       "      <td>-1.499937</td>\n",
       "      <td>1.499937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109148</th>\n",
       "      <td>BoSa</td>\n",
       "      <td>-1.498664</td>\n",
       "      <td>1.498664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>pleased</td>\n",
       "      <td>1.492500</td>\n",
       "      <td>1.492500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Fantastic</td>\n",
       "      <td>1.484813</td>\n",
       "      <td>1.484813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109147</th>\n",
       "      <td>hopes</td>\n",
       "      <td>-1.483347</td>\n",
       "      <td>1.483347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>loved</td>\n",
       "      <td>1.477041</td>\n",
       "      <td>1.477041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109146</th>\n",
       "      <td>Not</td>\n",
       "      <td>-1.475595</td>\n",
       "      <td>1.475595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109145</th>\n",
       "      <td>unimpressed</td>\n",
       "      <td>-1.475053</td>\n",
       "      <td>1.475053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109144</th>\n",
       "      <td>sick</td>\n",
       "      <td>-1.461775</td>\n",
       "      <td>1.461775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>1.460370</td>\n",
       "      <td>1.460370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>1.458760</td>\n",
       "      <td>1.458760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>delish</td>\n",
       "      <td>1.433576</td>\n",
       "      <td>1.433576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>BEST</td>\n",
       "      <td>1.433322</td>\n",
       "      <td>1.433322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109143</th>\n",
       "      <td>rotten</td>\n",
       "      <td>-1.432191</td>\n",
       "      <td>1.432191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Kudos</td>\n",
       "      <td>1.427076</td>\n",
       "      <td>1.427076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Thank</td>\n",
       "      <td>1.426586</td>\n",
       "      <td>1.426586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>friendly</td>\n",
       "      <td>1.421320</td>\n",
       "      <td>1.421320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Holy</td>\n",
       "      <td>1.420908</td>\n",
       "      <td>1.420908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word     coeff  abs_coeff\n",
       "109197           worst -3.440507   3.440507\n",
       "109196       poisoning -3.400705   3.400705\n",
       "0            Excellent  2.964825   2.964825\n",
       "109195           Worst -2.912159   2.912159\n",
       "1            excellent  2.787336   2.787336\n",
       "109194           bland -2.742936   2.742936\n",
       "109193            rude -2.735447   2.735447\n",
       "2                 Love  2.598844   2.598844\n",
       "109192   disappointing -2.565074   2.565074\n",
       "109191           stale -2.515708   2.515708\n",
       "3              Amazing  2.454324   2.454324\n",
       "109190        horrible -2.426339   2.426339\n",
       "4              perfect  2.407325   2.407325\n",
       "5            delicious  2.395399   2.395399\n",
       "109189      overpriced -2.363638   2.363638\n",
       "109188       tasteless -2.344295   2.344295\n",
       "109187           waste -2.324308   2.324308\n",
       "6              awesome  2.296141   2.296141\n",
       "109186           worse -2.272379   2.272379\n",
       "109185        mediocre -2.260581   2.260581\n",
       "109184        terrible -2.244452   2.244452\n",
       "109183           WORST -2.240749   2.240749\n",
       "7                 Best  2.239415   2.239415\n",
       "109182           sucks -2.231289   2.231289\n",
       "109181  unprofessional -2.217887   2.217887\n",
       "109180      disgusting -2.171601   2.171601\n",
       "8            Delicious  2.164957   2.164957\n",
       "9              Awesome  2.154144   2.154144\n",
       "10               Great  2.138344   2.138344\n",
       "11           fantastic  2.132356   2.132356\n",
       "...                ...       ...        ...\n",
       "109153        inedible -1.541175   1.541175\n",
       "26          definitely  1.539007   1.539007\n",
       "109152             NOT -1.537896   1.537896\n",
       "27            favorite  1.531097   1.531097\n",
       "28          disappoint  1.530840   1.530840\n",
       "29           flavorful  1.528547   1.528547\n",
       "30            fabulous  1.521572   1.521572\n",
       "31         outstanding  1.516476   1.516476\n",
       "32         exceptional  1.513732   1.513732\n",
       "109151         slowest -1.505931   1.505931\n",
       "109150             rip -1.503232   1.503232\n",
       "33                love  1.501592   1.501592\n",
       "109149           dirty -1.499937   1.499937\n",
       "109148            BoSa -1.498664   1.498664\n",
       "34             pleased  1.492500   1.492500\n",
       "35           Fantastic  1.484813   1.484813\n",
       "109147           hopes -1.483347   1.483347\n",
       "36               loved  1.477041   1.477041\n",
       "109146             Not -1.475595   1.475595\n",
       "109145     unimpressed -1.475053   1.475053\n",
       "109144            sick -1.461775   1.461775\n",
       "37           wonderful  1.460370   1.460370\n",
       "38           beautiful  1.458760   1.458760\n",
       "39              delish  1.433576   1.433576\n",
       "40                BEST  1.433322   1.433322\n",
       "109143          rotten -1.432191   1.432191\n",
       "41               Kudos  1.427076   1.427076\n",
       "42               Thank  1.426586   1.426586\n",
       "43            friendly  1.421320   1.421320\n",
       "44                Holy  1.420908   1.420908\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs.sort_values(by=\"abs_coeff\",inplace=True,ascending=False)\n",
    "feature_coeffs.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useless words (dont tell you anything about the polarity of the review):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "      <th>abs_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80030</th>\n",
       "      <td>magnetized</td>\n",
       "      <td>3.125496e-10</td>\n",
       "      <td>3.125496e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80051</th>\n",
       "      <td>Quiznoes</td>\n",
       "      <td>-2.742999e-10</td>\n",
       "      <td>2.742999e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80049</th>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>-2.656424e-10</td>\n",
       "      <td>2.656424e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80047</th>\n",
       "      <td>Caballo</td>\n",
       "      <td>-2.656424e-10</td>\n",
       "      <td>2.656424e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80048</th>\n",
       "      <td>Montañera</td>\n",
       "      <td>-2.656424e-10</td>\n",
       "      <td>2.656424e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80050</th>\n",
       "      <td>hogado</td>\n",
       "      <td>-2.656424e-10</td>\n",
       "      <td>2.656424e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80033</th>\n",
       "      <td>Pashley</td>\n",
       "      <td>1.789651e-10</td>\n",
       "      <td>1.789651e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80034</th>\n",
       "      <td>tapanade</td>\n",
       "      <td>1.233110e-10</td>\n",
       "      <td>1.233110e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80046</th>\n",
       "      <td>mircro</td>\n",
       "      <td>-1.002039e-10</td>\n",
       "      <td>1.002039e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80045</th>\n",
       "      <td>Clandestine</td>\n",
       "      <td>-9.795348e-11</td>\n",
       "      <td>9.795348e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80035</th>\n",
       "      <td>oversimplifying</td>\n",
       "      <td>8.948256e-11</td>\n",
       "      <td>8.948256e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80043</th>\n",
       "      <td>Wurstplatte</td>\n",
       "      <td>-7.289928e-11</td>\n",
       "      <td>7.289928e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80042</th>\n",
       "      <td>Hofbräu</td>\n",
       "      <td>-7.289928e-11</td>\n",
       "      <td>7.289928e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80044</th>\n",
       "      <td>Oktoberfestbier</td>\n",
       "      <td>-7.289928e-11</td>\n",
       "      <td>7.289928e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80041</th>\n",
       "      <td>roysrestaurant</td>\n",
       "      <td>-5.967515e-11</td>\n",
       "      <td>5.967515e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80036</th>\n",
       "      <td>Lambshanks</td>\n",
       "      <td>4.982600e-11</td>\n",
       "      <td>4.982600e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80037</th>\n",
       "      <td>Gougères</td>\n",
       "      <td>4.982600e-11</td>\n",
       "      <td>4.982600e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80038</th>\n",
       "      <td>lambshanks</td>\n",
       "      <td>4.982600e-11</td>\n",
       "      <td>4.982600e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80039</th>\n",
       "      <td>Boscaiola</td>\n",
       "      <td>-4.515086e-11</td>\n",
       "      <td>4.515086e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80040</th>\n",
       "      <td>boscaiola</td>\n",
       "      <td>-4.515086e-11</td>\n",
       "      <td>4.515086e-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word         coeff     abs_coeff\n",
       "80030       magnetized  3.125496e-10  3.125496e-10\n",
       "80051         Quiznoes -2.742999e-10  2.742999e-10\n",
       "80049    Miscellaneous -2.656424e-10  2.656424e-10\n",
       "80047          Caballo -2.656424e-10  2.656424e-10\n",
       "80048        Montañera -2.656424e-10  2.656424e-10\n",
       "80050           hogado -2.656424e-10  2.656424e-10\n",
       "80033          Pashley  1.789651e-10  1.789651e-10\n",
       "80034         tapanade  1.233110e-10  1.233110e-10\n",
       "80046           mircro -1.002039e-10  1.002039e-10\n",
       "80045      Clandestine -9.795348e-11  9.795348e-11\n",
       "80035  oversimplifying  8.948256e-11  8.948256e-11\n",
       "80043      Wurstplatte -7.289928e-11  7.289928e-11\n",
       "80042          Hofbräu -7.289928e-11  7.289928e-11\n",
       "80044  Oktoberfestbier -7.289928e-11  7.289928e-11\n",
       "80041   roysrestaurant -5.967515e-11  5.967515e-11\n",
       "80036       Lambshanks  4.982600e-11  4.982600e-11\n",
       "80037         Gougères  4.982600e-11  4.982600e-11\n",
       "80038       lambshanks  4.982600e-11  4.982600e-11\n",
       "80039        Boscaiola -4.515086e-11  4.515086e-11\n",
       "80040        boscaiola -4.515086e-11  4.515086e-11"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "      <th>abs_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109197</th>\n",
       "      <td>worst</td>\n",
       "      <td>-3.440507</td>\n",
       "      <td>3.440507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109196</th>\n",
       "      <td>poisoning</td>\n",
       "      <td>-3.400705</td>\n",
       "      <td>3.400705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excellent</td>\n",
       "      <td>2.964825</td>\n",
       "      <td>2.964825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109195</th>\n",
       "      <td>Worst</td>\n",
       "      <td>-2.912159</td>\n",
       "      <td>2.912159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>excellent</td>\n",
       "      <td>2.787336</td>\n",
       "      <td>2.787336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word     coeff  abs_coeff\n",
       "109197      worst -3.440507   3.440507\n",
       "109196  poisoning -3.400705   3.400705\n",
       "0       Excellent  2.964825   2.964825\n",
       "109195      Worst -2.912159   2.912159\n",
       "1       excellent  2.787336   2.787336"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines To Make CV/Transformations Easier With Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building pipelines here for transformations\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering a subset of features to make processing faster    \n",
    "yelp_pipeline = Pipeline([(\"countVect\", CountVectorizer(max_features = 1000)),(\"logreg\", LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_cv = StratifiedKFold(n_splits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   46.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9624156638118878"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(yelp_pipeline,\n",
    "                yelp_smaller.text,\n",
    "                yelp_smaller.target,\n",
    "                scoring=\"accuracy\",\n",
    "                cv=strat_cv,\n",
    "                n_jobs=-1,\n",
    "                verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countVect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_pipeline.fit(yelp_smaller.text,yelp_smaller.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that accepts a vectorizer and returns a table with the coefficients and accuracy of cv-ed model\n",
    "def tokenize_test(vect):\n",
    "    pipe = Pipeline([(\"vect\",vect),(\"lr\",LogisticRegression())])\n",
    "    pipe.fit(yelp_smaller.text,yelp_smaller.target)\n",
    "    num_features = len(pipe.steps[0][1].get_feature_names())\n",
    "    print('Num Features: ', num_features)\n",
    "\n",
    "    zipped_coeffs = list(zip(pipe.steps[0][1].get_feature_names(),\n",
    "                             pipe.steps[1][1].coef_[0]))\n",
    "    feature_coeffs = pd.DataFrame(zipped_coeffs,columns=[\"word\",\"coeff\"]).sort_values(by=\"coeff\",ascending=False)\n",
    "    feature_coeffs.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    strat_cv = StratifiedKFold(n_splits=2)\n",
    "    acc = np.mean(cross_val_score(pipe,\n",
    "                yelp_smaller.text,\n",
    "                yelp_smaller.target,\n",
    "                scoring=\"accuracy\",\n",
    "                cv=strat_cv,\n",
    "                n_jobs=-1,\n",
    "                verbose=1))\n",
    "    print(\"Accuracy:\", acc)\n",
    "    return (feature_coeffs, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features:  1000\n",
      "Accuracy: 0.9624156638118878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   46.0s finished\n"
     ]
    }
   ],
   "source": [
    "# We can customize this vec as we want\n",
    "vect = CountVectorizer(max_features=1000)\n",
    "feature_coeffs,acc = tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>perfection</td>\n",
       "      <td>3.142748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>excellent</td>\n",
       "      <td>2.518639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>incredible</td>\n",
       "      <td>2.454513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>delicious</td>\n",
       "      <td>2.364826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>perfect</td>\n",
       "      <td>2.263292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fantastic</td>\n",
       "      <td>2.164809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amazing</td>\n",
       "      <td>2.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>awesome</td>\n",
       "      <td>2.135797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>favorites</td>\n",
       "      <td>2.039298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>outstanding</td>\n",
       "      <td>1.998869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word     coeff\n",
       "0   perfection  3.142748\n",
       "1    excellent  2.518639\n",
       "2   incredible  2.454513\n",
       "3    delicious  2.364826\n",
       "4      perfect  2.263292\n",
       "5    fantastic  2.164809\n",
       "6      amazing  2.159500\n",
       "7      awesome  2.135797\n",
       "8    favorites  2.039298\n",
       "9  outstanding  1.998869"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>worst</td>\n",
       "      <td>-3.415490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>bland</td>\n",
       "      <td>-2.805640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>terrible</td>\n",
       "      <td>-2.641418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>horrible</td>\n",
       "      <td>-2.565443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>rude</td>\n",
       "      <td>-2.414776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>poor</td>\n",
       "      <td>-2.030622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>unfortunately</td>\n",
       "      <td>-1.789316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>dirty</td>\n",
       "      <td>-1.546174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>sorry</td>\n",
       "      <td>-1.490507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>tasted</td>\n",
       "      <td>-1.272886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word     coeff\n",
       "999          worst -3.415490\n",
       "998          bland -2.805640\n",
       "997       terrible -2.641418\n",
       "996       horrible -2.565443\n",
       "995           rude -2.414776\n",
       "994           poor -2.030622\n",
       "993  unfortunately -1.789316\n",
       "992          dirty -1.546174\n",
       "991          sorry -1.490507\n",
       "990         tasted -1.272886"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs.sort_values(\"coeff\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features:  1000\n",
      "Accuracy: 0.957789092910986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   37.5s finished\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words='english',max_features=1000, min_df=0.005, max_df = 0.7)\n",
    "feature_coeffs,acc = tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>perfection</td>\n",
       "      <td>3.526148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>incredible</td>\n",
       "      <td>2.572324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>excellent</td>\n",
       "      <td>2.571913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phenomenal</td>\n",
       "      <td>2.459257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>delicious</td>\n",
       "      <td>2.403472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>delish</td>\n",
       "      <td>2.284492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fantastic</td>\n",
       "      <td>2.216050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>amazing</td>\n",
       "      <td>2.188376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>awesome</td>\n",
       "      <td>2.170381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>notch</td>\n",
       "      <td>2.162207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word     coeff\n",
       "0  perfection  3.526148\n",
       "1  incredible  2.572324\n",
       "2   excellent  2.571913\n",
       "3  phenomenal  2.459257\n",
       "4   delicious  2.403472\n",
       "5      delish  2.284492\n",
       "6   fantastic  2.216050\n",
       "7     amazing  2.188376\n",
       "8     awesome  2.170381\n",
       "9       notch  2.162207"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>worst</td>\n",
       "      <td>-3.425821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>bland</td>\n",
       "      <td>-2.772050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>horrible</td>\n",
       "      <td>-2.671436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>awful</td>\n",
       "      <td>-2.648629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>terrible</td>\n",
       "      <td>-2.633795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>rude</td>\n",
       "      <td>-2.358765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>waste</td>\n",
       "      <td>-2.278950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>poor</td>\n",
       "      <td>-2.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>unfortunately</td>\n",
       "      <td>-1.832402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>dirty</td>\n",
       "      <td>-1.599713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word     coeff\n",
       "999          worst -3.425821\n",
       "998          bland -2.772050\n",
       "997       horrible -2.671436\n",
       "996          awful -2.648629\n",
       "995       terrible -2.633795\n",
       "994           rude -2.358765\n",
       "993          waste -2.278950\n",
       "992           poor -2.092000\n",
       "991  unfortunately -1.832402\n",
       "990          dirty -1.599713"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coeffs.tail(10).sort_values(\"coeff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'too', 'sometimes', 'whereas', 'forty', 'because', 'ie', 'most', 'seem', 'found', 'always', 'my', 'became', 'once', 'without', 'take', 'behind', 'anyone', 'nobody', 'hasnt', 'former', 'less', 'un', 'fill', 'never', 'more', 'least', 'seeming', 'then', 'bottom', 'ten', 'anywhere', 'find', 'upon', 'rather', 'beside', 'it', 'under', 'whatever', 'whoever', 'still', 'could', 'nevertheless', 'they', 'why', 'such', 'moreover', 'even', 'over', 'since', 'around', 'for', 'mine', 'six', 'wherever', 'herein', 'do', 'yourself', 'a', 'somehow', 'nine', 'someone', 'five', 'your', 'ours', 'and', 'him', 'done', 'part', 'hence', 'few', 'these', 'move', 'would', 'fire', 're', 'anyhow', 'latterly', 'we', 'first', 'somewhere', 'the', 'therein', 'everywhere', 'other', 'whereby', 'thence', 'noone', 'an', 'everyone', 'seems', 'whence', 'with', 'within', 'three', 'has', 'many', 'among', 'describe', 'whether', 'co', 'now', 'this', 'was', 'while', 'you', 'during', 'via', 'although', 'its', 'very', 'eg', 'any', 'either', 'through', 'back', 'hers', 'hundred', 'each', 'again', 'are', 'all', 'becomes', 'besides', 'may', 'one', 'our', 'between', 'due', 'fifty', 'eight', 'them', 'cant', 'of', 'per', 'until', 'be', 'last', 'might', 'fifteen', 'mill', 'name', 'full', 'same', 'if', 'often', 'however', 'no', 'there', 'else', 'from', 'which', 'top', 'seemed', 'whereupon', 'twelve', 'out', 'nowhere', 'that', 'system', 'beforehand', 'is', 'had', 'every', 'anyway', 'couldnt', 'have', 'herself', 'twenty', 'perhaps', 'yours', 'otherwise', 'sincere', 'been', 'am', 'thin', 'those', 'their', 'elsewhere', 'alone', 'next', 'on', 'amount', 'here', 'namely', 'she', 'thereby', 'thick', 'hereafter', 'enough', 'hereupon', 'towards', 'afterwards', 'etc', 'wherein', 'in', 'bill', 'beyond', 'show', 'only', 'everything', 'onto', 'others', 'several', 'some', 'interest', 'meanwhile', 'myself', 'anything', 'side', 'he', 'not', 'toward', 'above', 'mostly', 'i', 'thru', 'down', 'together', 'will', 'two', 'into', 'how', 'serious', 'off', 'becoming', 'himself', 'at', 'ltd', 'also', 'none', 'yourselves', 'whither', 'except', 'up', 'us', 'to', 'indeed', 'cry', 'see', 'when', 'her', 'something', 'already', 'de', 'so', 'who', 'give', 'being', 'were', 'almost', 'amoungst', 'whom', 'eleven', 'whenever', 'than', 'where', 'about', 'become', 'neither', 'hereby', 'sixty', 'along', 'sometime', 'put', 'whereafter', 'thereupon', 'whose', 'or', 'nor', 'itself', 'throughout', 'yet', 'keep', 'own', 'but', 'by', 'made', 'thereafter', 'must', 'his', 'another', 'con', 'themselves', 'third', 'thus', 'as', 'whole', 'therefore', 'before', 'both', 'ever', 'inc', 'much', 'can', 'cannot', 'formerly', 'front', 'four', 'further', 'get', 'go', 'should', 'below', 'nothing', 'what', 'me', 'latter', 'amongst', 'detail', 'please', 'call', 'across', 'empty', 'ourselves', 'well', 'against', 'though', 'after'})\n"
     ]
    }
   ],
   "source": [
    "# set of stop words\n",
    "print(vect.get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other CountVectorizer Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features:  1581\n",
      "Accuracy: 0.9623716918424552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   38.6s finished\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words and only keep words appearing in 0.5% of documents\n",
    "vect = CountVectorizer(stop_words='english', min_df = 0.005)\n",
    "features,acc = tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>perfection</td>\n",
       "      <td>3.163696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>incredible</td>\n",
       "      <td>2.679297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phenomenal</td>\n",
       "      <td>2.606023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>excellent</td>\n",
       "      <td>2.605894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pleasantly</td>\n",
       "      <td>2.485137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word     coeff\n",
       "0  perfection  3.163696\n",
       "1  incredible  2.679297\n",
       "2  phenomenal  2.606023\n",
       "3   excellent  2.605894\n",
       "4  pleasantly  2.485137"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>worst</td>\n",
       "      <td>-3.420313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>disappointing</td>\n",
       "      <td>-2.992058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>disappointment</td>\n",
       "      <td>-2.850296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>disgusting</td>\n",
       "      <td>-2.719061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>horrible</td>\n",
       "      <td>-2.676040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word     coeff\n",
       "1580           worst -3.420313\n",
       "1579   disappointing -2.992058\n",
       "1578  disappointment -2.850296\n",
       "1577      disgusting -2.719061\n",
       "1576        horrible -2.676040"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.sort_values(\"coeff\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>perfection</td>\n",
       "      <td>3.163696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>incredible</td>\n",
       "      <td>2.679297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phenomenal</td>\n",
       "      <td>2.606023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>excellent</td>\n",
       "      <td>2.605894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pleasantly</td>\n",
       "      <td>2.485137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word     coeff\n",
       "0  perfection  3.163696\n",
       "1  incredible  2.679297\n",
       "2  phenomenal  2.606023\n",
       "3   excellent  2.605894\n",
       "4  pleasantly  2.485137"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>worst</td>\n",
       "      <td>-3.420313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>disappointing</td>\n",
       "      <td>-2.992058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>disappointment</td>\n",
       "      <td>-2.850296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>disgusting</td>\n",
       "      <td>-2.719061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>horrible</td>\n",
       "      <td>-2.676040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word     coeff\n",
       "1580           worst -3.420313\n",
       "1579   disappointing -2.992058\n",
       "1578  disappointment -2.850296\n",
       "1577      disgusting -2.719061\n",
       "1576        horrible -2.676040"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.sort_values(\"coeff\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF to Summarize a Yelp Review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113691, 87434)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a document-term matrix using TF-IDF\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "dtm = vect.fit_transform(yelp_smaller.text)\n",
    "features = vect.get_feature_names()\n",
    "dtm.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def summarize():\n",
    "    \n",
    "    # choose a random review that is at least 500 characters\n",
    "    review_length = 0\n",
    "    while review_length < 500:\n",
    "        review_id = np.random.randint(0, len(yelp_smaller))\n",
    "        review_text = yelp_smaller.text.iloc[review_id]\n",
    "        review_length = len(review_text)\n",
    "    \n",
    "    # create a dictionary of words and their TF-IDF scores\n",
    "    word_scores = {}\n",
    "    for word in TextBlob(review_text).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[review_id, features.index(word)]\n",
    "    \n",
    "    # print words with the top 5 TF-IDF scores\n",
    "    print('TOP SCORING WORDS:')\n",
    "    top_scores = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, score in top_scores:\n",
    "        print(word)\n",
    "    \n",
    "    # print 5 random words\n",
    "    print('\\n' + 'RANDOM WORDS:')\n",
    "    random_words = np.random.choice(list(word_scores.keys()), size=5, replace=False)\n",
    "    for word in random_words:\n",
    "        print(word)\n",
    "    \n",
    "    # print the review\n",
    "    print('\\n' + review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP SCORING WORDS:\n",
      "cioccolatta\n",
      "panino\n",
      "godly\n",
      "porchetta\n",
      "darren\n",
      "\n",
      "RANDOM WORDS:\n",
      "torta\n",
      "boss\n",
      "apple\n",
      "home\n",
      "time\n",
      "\n",
      "Our boss treated some co-workers and myself to lunch here last week (one of my colleagues was leaving) and had a terrific time. The decor is amazing and so is the food. I had the Porchetta panino (pulled pork Shoulder with horseradish apple salsa) which came with these parsely-seasoned home fries. All of it was amazing. Like Darren (I was so jealous he got to go there back in February and I didn't), I had the Torta di Cioccolatta, and it was godly. The service was on-target too. I love this place and that it's so close to where I now work. Seriously gourmet.\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'design', 'poor', 'signag', 'bad', 'attitud', 'from', 'employe', 'tsa', 'ineffect', 'to', 'the', 'point', 'of', 'danger', 'dirti', 'bathroom', 'horrend', 'access', 'deplor', 'all', 'around']\n"
     ]
    }
   ],
   "source": [
    "# initialize stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "review_id = np.random.randint(0, len(yelp_smaller))\n",
    "review = TextBlob(yelp_smaller.text.iloc[review_id])\n",
    "\n",
    "# stem each word\n",
    "print([stemmer.stem(word) for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "- **What:** Derive the canonical form ('lemma') of a word\n",
    "- **Why:** Can be better than stemming\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bad', 'design', 'Poor', 'signage', 'Bad', 'attitude', 'from', 'employee', 'TSA', 'ineffective', 'to', 'the', 'point', 'of', 'danger', 'Dirty', 'bathroom', 'Horrendous', 'accessibility', 'Deplorable', 'all', 'around']\n"
     ]
    }
   ],
   "source": [
    "# assume every word is a noun\n",
    "print([word.lemmatize() for word in review.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume every word is a verb\n",
    "print([word.lemmatize(pos='v') for word in review.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of lemmas\n",
    "def split_into_lemmas(text):\n",
    "    text = text.lower()\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-349396b5e359>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# use split_into_lemmas as the feature extraction function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit_into_lemmas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-6cea85a04c86>\u001b[0m in \u001b[0;36mtokenize_test\u001b[1;34m(vect)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vect\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lr\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myelp_smaller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myelp_smaller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mnum_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Num Features: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[0;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                        **fit_params):\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-6a2ce19afcff>\u001b[0m in \u001b[0;36msplit_into_lemmas\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msplit_into_lemmas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\decorators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mWordList\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mWordList\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mword\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \"\"\"\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mWordList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_punc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, collection)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mits\u001b[0m \u001b[0monly\u001b[0m \u001b[0margument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \"\"\"\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWordList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollection\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mits\u001b[0m \u001b[0monly\u001b[0m \u001b[0margument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \"\"\"\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWordList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollection\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\tokenizers.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     71\u001b[0m         _word_tokenizer.itokenize(sentence, include_punc=include_punc,\n\u001b[0;32m     72\u001b[0m                                 *args, **kwargs)\n\u001b[1;32m---> 73\u001b[1;33m         for sentence in sent_tokenize(text))\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\base.py\u001b[0m in \u001b[0;36mitokenize\u001b[1;34m(self, text, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \"\"\"\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m##### SENTIMENT ANALYZERS ####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\tokenizers.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, include_punc)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0minclude_punc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moptional\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mto\u001b[0m \u001b[0minclude\u001b[0m \u001b[0mpunctuation\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mseparate\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefault\u001b[0m \u001b[0mto\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         '''\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minclude_punc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m     return [token for sent in sentences\n\u001b[0m\u001b[0;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m--> 130\u001b[1;33m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr' \\1 \\2 '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr' \\1 \\2 '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use split_into_lemmas as the feature extraction function\n",
    "vect = CountVectorizer(analyzer=split_into_lemmas)\n",
    "features,acc = tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.sort_values(\"coeff\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad design. Poor signage.  Bad attitude from employees.  TSA ineffective to the point of danger.  Dirty bathrooms.  Horrendous accessibility. Deplorable all around.\n"
     ]
    }
   ],
   "source": [
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# polarity ranges from -1 (most negative) to 1 (most positive)\n",
    "review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_smaller['length'] = yelp_smaller.text.apply(len)\n",
    "#Picking a subset of 250 reviews for sentiment analysis\n",
    "yelp_smaller_subset = yelp_smaller.iloc[1:250, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns the polarity\n",
    "def detect_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new DataFrame column for sentiment\n",
    "yelp_smaller['sentiment'] = yelp_smaller_subset.text.apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x45bb31d0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# boxplot of sentiment grouped by stars\n",
    "yelp_smaller.boxplot(column='sentiment', by='stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Adding the extra feature 'length' to the document term matrix\n",
    "\n",
    "feature_cols = ['text','length']\n",
    "X = yelp_smaller[feature_cols]\n",
    "y = yelp_smaller.target\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-4a1da257c13e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrat_cv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                         \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                         verbose=1))\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text only accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_only_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             return_times=True)\n\u001b[1;32m--> 206\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use CountVectorizer with text column only\n",
    "yelp_pipeline = Pipeline([(\"countVect\",CountVectorizer(stop_words=\"english\")),\n",
    "                     (\"lr\",LogisticRegression(C=0.1))])\n",
    "strat_cv = StratifiedKFold(n_splits=2)\n",
    "text_only_acc = np.mean(cross_val_score(yelp_pipeline,\n",
    "                        X.text,\n",
    "                        y,\n",
    "                        scoring=\"accuracy\",\n",
    "                        cv=strat_cv,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=1))\n",
    "print(\"text only accuracy: \",np.round(text_only_acc,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original matrix: \",CountVectorizer(stop_words=\"english\").fit_transform(X.text).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Word2vec techniques on the yelp dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods I have used till now involve frequency embedding techniques. I want to try and improve the accuracy of my sentiment analysis by using contextual embedding techniques on a preprocessed dataset. By preprocessing, I mean removal of stopwords and punctuation and so on. Finally I want to try building a convolutional neural network model with feature extraction using word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Madhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import base64\n",
    "import string #use for punctuation removal\n",
    "import re\n",
    "from collections import Counter\n",
    "from time import time\n",
    "\n",
    "#nlp specific libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8Gi91EkhG2_CJLk_WqjpEg</td>\n",
       "      <td>1</td>\n",
       "      <td>I found them through Craigslist. I did the 2 m...</td>\n",
       "      <td>um0ITBTHoohdozmFA6snlw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6zincCvQTb9BsuK-GHZ4cw</td>\n",
       "      <td>5</td>\n",
       "      <td>Great pizza, excellent delivery time &amp; if I ev...</td>\n",
       "      <td>tNcIhWEeAl607ENbeRSm0w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kzANsUYAZFnMsogB1RFE7A</td>\n",
       "      <td>5</td>\n",
       "      <td>Finding an incredible hairstylist is like find...</td>\n",
       "      <td>EJ7ZhRHsMWj8du77LX34gw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uBAHE6QOJlndILPIoeEpCQ</td>\n",
       "      <td>1</td>\n",
       "      <td>I attempted to use a coupon for the exact item...</td>\n",
       "      <td>RNVCLdKNddXp8v-y6Aq2Xg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yzmvsG3Vo-2_F3_0wHzJdA</td>\n",
       "      <td>1</td>\n",
       "      <td>Dammit! I had such high hopes for this place. ...</td>\n",
       "      <td>D5ETbJC0dptWR07sbTlqRg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  stars  \\\n",
       "0  8Gi91EkhG2_CJLk_WqjpEg      1   \n",
       "1  6zincCvQTb9BsuK-GHZ4cw      5   \n",
       "2  kzANsUYAZFnMsogB1RFE7A      5   \n",
       "3  uBAHE6QOJlndILPIoeEpCQ      1   \n",
       "4  yzmvsG3Vo-2_F3_0wHzJdA      1   \n",
       "\n",
       "                                                text                 user_id  \n",
       "0  I found them through Craigslist. I did the 2 m...  um0ITBTHoohdozmFA6snlw  \n",
       "1  Great pizza, excellent delivery time & if I ev...  tNcIhWEeAl607ENbeRSm0w  \n",
       "2  Finding an incredible hairstylist is like find...  EJ7ZhRHsMWj8du77LX34gw  \n",
       "3  I attempted to use a coupon for the exact item...  RNVCLdKNddXp8v-y6Aq2Xg  \n",
       "4  Dammit! I had such high hopes for this place. ...  D5ETbJC0dptWR07sbTlqRg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For word2vec techniques which are computationally very expensive, I am going to use a smaller subset of the original dataset. \n",
    "#This subset contains 20,000 rows of data. It only have reviews with 5 or 1 stars.\n",
    "yelp = pd.read_csv(\"yelp_smaller_20k.csv\",index_col=0).reset_index(drop=True)\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    15074\n",
      "1     4926\n",
      "Name: stars, dtype: int64\n",
      "5    0.7537\n",
      "1    0.2463\n",
      "Name: stars, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(yelp.stars.value_counts())\n",
    "\n",
    "print(yelp.stars.value_counts()/len(yelp.stars))\n",
    "\n",
    "#We can see this dataset is highly imbalanced. We need to deal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target\n",
    "yelp[\"target\"] = (yelp.stars == 5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing\n",
    "\n",
    "Below, I perform pre-processing on the text data. The steps include 1) stopword removal 2)lemmatization 3) Removal of punctuation 4) splitting text into sentences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to preprocess text for a word2vec model\n",
    "def cleanup_text_word2vec(docs, logging=False):\n",
    "    sentences = []\n",
    "    counter = 1\n",
    "    for doc in nlp.pipe(docs,n_threads=4,disable=[\"tagger\"]):\n",
    "        if counter % 1000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents\" % (counter, len(docs)))\n",
    "        # Grab lemmatized form of words and make lowercase\n",
    "        doc = \" \".join([tok.lemma_.lower() for tok in doc])\n",
    "        # Split into sentences based on punctuation - assuming a period, question mark, exclamation point, and apostroper\n",
    "        # dictate the end of a given sentence.\n",
    "        doc = re.split(\"[\\.?!;] \", doc)\n",
    "        # Remove commas, periods, and other punctuation (mostly commas) from each sentence\n",
    "        doc = [sent.translate(sent.maketrans(\"\",\"\",string.punctuation)) for sent in doc]\n",
    "        # Split each sentence into distinct words\n",
    "        doc = [sent.split() for sent in doc]\n",
    "        sentences += doc\n",
    "        counter += 1\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 out of 20000 documents\n",
      "Processed 2000 out of 20000 documents\n",
      "Processed 3000 out of 20000 documents\n",
      "Processed 4000 out of 20000 documents\n",
      "Processed 5000 out of 20000 documents\n",
      "Processed 6000 out of 20000 documents\n",
      "Processed 7000 out of 20000 documents\n",
      "Processed 8000 out of 20000 documents\n",
      "Processed 9000 out of 20000 documents\n",
      "Processed 10000 out of 20000 documents\n",
      "Processed 11000 out of 20000 documents\n",
      "Processed 12000 out of 20000 documents\n",
      "Processed 13000 out of 20000 documents\n",
      "Processed 14000 out of 20000 documents\n",
      "Processed 15000 out of 20000 documents\n",
      "Processed 16000 out of 20000 documents\n",
      "Processed 17000 out of 20000 documents\n",
      "Processed 18000 out of 20000 documents\n",
      "Processed 19000 out of 20000 documents\n",
      "Processed 20000 out of 20000 documents\n"
     ]
    }
   ],
   "source": [
    "yelp_cleaned_word2vec = cleanup_text_word2vec(yelp.text, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Madhu\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13794 unique words represented by 300 dimensional vectors in this model\n"
     ]
    }
   ],
   "source": [
    "#Building a word2vec model\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "text_dim = 300\n",
    "\n",
    "\n",
    "#Using a skip-gram model, so sg = 1\n",
    "wordvec_model_300 = Word2Vec(yelp_cleaned_word2vec, size=text_dim, window=5, min_count=3, workers=4, sg=1)\n",
    "\n",
    "print(\"%d unique words represented by %d dimensional vectors in this model\" % (len(wordvec_model_300.wv.vocab), text_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Madhu\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to cafe:\n",
      " [('café', 0.7513088583946228), ('bakery', 0.7152332067489624), ('pub', 0.7121677398681641), ('tavern', 0.7121496796607971), ('uptown', 0.6944003701210022), ('theatre', 0.6932998895645142), ('brewery', 0.6894760131835938), ('cabo', 0.6865200400352478), ('eatery', 0.6861717104911804), ('bistro', 0.6799688339233398)]\n",
      "Which of the words amazing good awesome bland don't match:  bland\n",
      "\n",
      "Words most similar to restaurant - big:\n",
      " [('cuisine', 1.4078543186187744), ('establishment', 1.3695553541183472), ('la', 1.352676510810852), ('salon', 1.328722596168518), ('cafe', 1.3157135248184204), ('place', 1.2921502590179443), ('dealership', 1.2884690761566162), ('property', 1.287079930305481), ('hotel', 1.2846736907958984), ('fitness', 1.2845988273620605)]\n",
      "\n",
      "How related is morning to dinner:  0.4116895\n",
      "\n",
      "How related is night to dinner:  0.6540447\n"
     ]
    }
   ],
   "source": [
    "#Looking into the wordtovec model created above\n",
    "\n",
    "print(\"Words most similar to cafe:\\n\",wordvec_model_300.wv.most_similar(positive=['cafe']))\n",
    "\n",
    "print(\"Which of the words amazing good awesome bland don\\'t match: \",wordvec_model_300.wv.doesnt_match(\"amazing good awesome bland\".split()))\n",
    "print()\n",
    "\n",
    "print(\"Words most similar to restaurant - big:\\n\",wordvec_model_300.wv.most_similar_cosmul(positive=['restaurant'], negative=[\"big\"]))\n",
    "print()\n",
    "\n",
    "print(\"How related is morning to dinner: \",wordvec_model_300.wv.similarity('morning', 'dinner'))\n",
    "print()\n",
    "print(\"How related is night to dinner: \",wordvec_model_300.wv.similarity('night', 'dinner'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document classification using Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For document classification into positive or negative using Word2Vec, I first create a function to compute the average of all the word vectors in a piece of text. After this, I would clean the text in such a way that the information about which reviews each of the sentences belong to is preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute average function\n",
    "\n",
    "def create_average_vec(doc):\n",
    "    average = np.zeros((text_dim,), dtype = 'float32')\n",
    "    num_words = 0\n",
    "    words = doc.split(\" \")\n",
    "    \n",
    "    for word in words:\n",
    "        if word in wordvec_model_300.wv.vocab:\n",
    "            average = np.add(wordvec_model_300[word], average)\n",
    "            num_words+= 1\n",
    "    if num_words > 0:\n",
    "        average = np.divide(average,num_words)\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we need to re-process the input data such that we retain the indices of each doc. This is so we can apply the create average function on each document in the data\n",
    "\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def cleanup_text(docs, logging=True):\n",
    "    proc_docs = []\n",
    "    counter = 1\n",
    "    for doc in nlp.pipe(docs, n_threads=4,disable=[\"tagger\"]):\n",
    "        if counter % 200 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "        counter += 1\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc] #lemmatize, remove pronouns #if tok.lemma_ != '-PRON-'\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords.words() and tok not in punctuations] #remove stopwords, punctuation\n",
    "        tokens = ' '.join(tokens)\n",
    "        proc_docs.append(tokens)\n",
    "    return proc_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now applying the above function to yelp data. This takes a lot of time to run. So I run it once and now load in the dataset each time\n",
    "\n",
    "#yelp_smaller_clean = pd.Series(cleanup_text(yelp.text,logging=True))\n",
    "\n",
    "# LOAD IN THE CLEANED DATA INSTEAD :)\n",
    "yelp_smaller_clean = pd.read_csv(\"./yelp_cleaned_word2vec.csv\")\n",
    "yelp_smaller_clean.cleaned_text.fillna(\"\",inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Madhu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final word vector shape: (20000, 300)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>find craigslist 2 maid 68 dollar two hour firs...</td>\n",
       "      <td>I found them through Craigslist. I did the 2 m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great pizza excellent delivery time ever back ...</td>\n",
       "      <td>Great pizza, excellent delivery time &amp; if I ev...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finding incredible hairstylist like find unico...</td>\n",
       "      <td>Finding an incredible hairstylist is like find...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>attempt use coupon exact item state picture ma...</td>\n",
       "      <td>I attempted to use a coupon for the exact item...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dammit high hope place -pron- eat month finall...</td>\n",
       "      <td>Dammit! I had such high hopes for this place. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  \\\n",
       "0  find craigslist 2 maid 68 dollar two hour firs...   \n",
       "1  great pizza excellent delivery time ever back ...   \n",
       "2  finding incredible hairstylist like find unico...   \n",
       "3  attempt use coupon exact item state picture ma...   \n",
       "4  dammit high hope place -pron- eat month finall...   \n",
       "\n",
       "                                                text  stars  target  \n",
       "0  I found them through Craigslist. I did the 2 m...      1       0  \n",
       "1  Great pizza, excellent delivery time & if I ev...      5       1  \n",
       "2  Finding an incredible hairstylist is like find...      5       1  \n",
       "3  I attempted to use a coupon for the exact item...      1       0  \n",
       "4  Dammit! I had such high hopes for this place. ...      1       0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying the create average vector of words function to each document here\n",
    "final_cleaned_vec = np.zeros((yelp_smaller_clean.shape[0], text_dim), dtype=\"float32\")  # 20000 x 300\n",
    "for i in range(yelp_smaller_clean.shape[0]):\n",
    "    final_cleaned_vec[i] = create_average_vec(yelp_smaller_clean.cleaned_text.values[i])\n",
    "\n",
    "print(\"Final word vector shape:\", final_cleaned_vec.shape)\n",
    "yelp_smaller_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9470494144123537\n"
     ]
    }
   ],
   "source": [
    "# Now, we can use this 20000*300 vector as an input to a linear model and a non-linear model. Starting off with Logistic regression\n",
    "#Finding the cross validated score of Logistic regression using Stratified K-Fold\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold\n",
    "sk = StratifiedKFold(n_splits = 10)\n",
    "\n",
    "print(np.mean(cross_val_score(LogisticRegression(), final_cleaned_vec, yelp_smaller_clean.target, cv= sk, scoring = \"accuracy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9253499146224807"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a Random Forest Classifier, we can see that the accuracy of the simpler(logistic) model is more than the Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sk = StratifiedKFold(n_splits=3)\n",
    "np.mean(cross_val_score(RandomForestClassifier(n_estimators=50),\n",
    "                        final_cleaned_vec,\n",
    "                        yelp_smaller_clean.target,\n",
    "                        cv=sk,\n",
    "                        scoring=\"accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the last step of this project, we can train an MLP and Convolutional Neural Network Model on the data to see if there is an improvement in the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: (16000, 300)\n",
      "X_test size: (4000, 300)\n",
      "y_train size: (16000,)\n",
      "y_test size: (4000,)\n",
      "y-split train:\n",
      " 1    0.75575\n",
      "0    0.24425\n",
      "Name: target, dtype: float64\n",
      "y-split test:\n",
      " 1    0.7455\n",
      "0    0.2545\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_cleaned_vec, yelp_smaller_clean.target, test_size=0.2, random_state=21)\n",
    "\n",
    "print('X_train size: {}'.format(X_train.shape))\n",
    "print('X_test size: {}'.format(X_test.shape))\n",
    "print('y_train size: {}'.format(y_train.shape))\n",
    "print('y_test size: {}'.format(y_test.shape))\n",
    "print(\"y-split train:\\n\",y_train.value_counts()/y_train.shape[0])\n",
    "print(\"y-split test:\\n\",y_test.value_counts()/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Madhu\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "#Below, I build a Multi Layered Perceptron (Perceptrons with dropout at each step to prevent overfitting followed by sigmoid activation layer) and CNN (Convolution Filter + Relu Activation followed by Max pooling (and maybe dropout)) model followed by sigmoid activation)\n",
    "\n",
    "def build_deeplearning_model(model_type = \"mlp\", input_dim = 300):\n",
    "    model = Sequential()\n",
    "    if model_type == \"mlp\":\n",
    "        model.add(Dense(512, activation='relu', kernel_initializer='he_normal', input_dim=input_dim))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(512, activation='relu', kernel_initializer='he_normal', input_dim=input_dim))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(512, activation='relu', kernel_initializer='he_normal', input_dim=input_dim))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(512, activation='relu', kernel_initializer='he_normal', input_dim=input_dim))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation = 'sigmoid'))\n",
    "    elif model_type == \"cnn\":\n",
    "        inputs = Input(shape=(input_dim,1))\n",
    "        x = Conv1D(64, 3, strides=1, padding='same', kernel_initializer=\"uniform\",activation='relu')(inputs)\n",
    "        x = MaxPooling1D(pool_size=2)(x)\n",
    "        x = Conv1D(128, 3, strides=1, padding='same', kernel_initializer=\"uniform\",activation='relu')(x)\n",
    "        x = GlobalMaxPooling1D()(x) \n",
    "        outputs = Dense(1, kernel_initializer=\"uniform\", activation='sigmoid')(x)\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='CNN')\n",
    "    return model\n",
    "    \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text train shape:  (16000, 300, 1)\n",
      "Text test shape:  (4000, 300, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 300, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 150, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 150, 128)          24704     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 25,089\n",
      "Trainable params: 25,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Building the model\n",
    "\n",
    "model = build_deeplearning_model('cnn')\n",
    "\n",
    "# If the model is a CNN then expand the dimensions of the training data\n",
    "if model.name == \"CNN\":\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    print('Text train shape: ', X_train.shape)\n",
    "    print('Text test shape: ', X_test.shape)\n",
    "    \n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "16000/16000 [==============================] - ETA: 8:47 - loss: 0.6931 - acc: 0.601 - ETA: 6:26 - loss: 0.6919 - acc: 0.691 - ETA: 5:53 - loss: 0.6908 - acc: 0.718 - ETA: 5:46 - loss: 0.6903 - acc: 0.712 - ETA: 5:34 - loss: 0.6890 - acc: 0.726 - ETA: 5:32 - loss: 0.6874 - acc: 0.747 - ETA: 5:27 - loss: 0.6865 - acc: 0.745 - ETA: 5:17 - loss: 0.6855 - acc: 0.745 - ETA: 5:09 - loss: 0.6844 - acc: 0.744 - ETA: 4:53 - loss: 0.6831 - acc: 0.746 - ETA: 4:44 - loss: 0.6819 - acc: 0.745 - ETA: 4:35 - loss: 0.6805 - acc: 0.744 - ETA: 4:29 - loss: 0.6784 - acc: 0.748 - ETA: 4:28 - loss: 0.6767 - acc: 0.748 - ETA: 4:23 - loss: 0.6740 - acc: 0.753 - ETA: 4:16 - loss: 0.6721 - acc: 0.752 - ETA: 4:13 - loss: 0.6706 - acc: 0.750 - ETA: 4:06 - loss: 0.6680 - acc: 0.752 - ETA: 4:02 - loss: 0.6655 - acc: 0.752 - ETA: 3:56 - loss: 0.6627 - acc: 0.753 - ETA: 3:53 - loss: 0.6598 - acc: 0.754 - ETA: 3:51 - loss: 0.6558 - acc: 0.757 - ETA: 3:47 - loss: 0.6534 - acc: 0.757 - ETA: 3:44 - loss: 0.6521 - acc: 0.754 - ETA: 3:39 - loss: 0.6512 - acc: 0.751 - ETA: 3:37 - loss: 0.6484 - acc: 0.751 - ETA: 3:34 - loss: 0.6452 - acc: 0.752 - ETA: 3:31 - loss: 0.6458 - acc: 0.748 - ETA: 3:27 - loss: 0.6436 - acc: 0.747 - ETA: 3:26 - loss: 0.6415 - acc: 0.747 - ETA: 3:23 - loss: 0.6402 - acc: 0.746 - ETA: 3:21 - loss: 0.6378 - acc: 0.746 - ETA: 3:18 - loss: 0.6331 - acc: 0.748 - ETA: 3:14 - loss: 0.6311 - acc: 0.748 - ETA: 3:12 - loss: 0.6273 - acc: 0.750 - ETA: 3:10 - loss: 0.6255 - acc: 0.750 - ETA: 3:08 - loss: 0.6250 - acc: 0.749 - ETA: 3:06 - loss: 0.6246 - acc: 0.748 - ETA: 3:04 - loss: 0.6237 - acc: 0.748 - ETA: 3:02 - loss: 0.6228 - acc: 0.747 - ETA: 2:59 - loss: 0.6219 - acc: 0.747 - ETA: 2:57 - loss: 0.6196 - acc: 0.748 - ETA: 2:54 - loss: 0.6183 - acc: 0.748 - ETA: 2:52 - loss: 0.6173 - acc: 0.748 - ETA: 2:48 - loss: 0.6162 - acc: 0.748 - ETA: 2:47 - loss: 0.6132 - acc: 0.749 - ETA: 2:45 - loss: 0.6133 - acc: 0.748 - ETA: 2:42 - loss: 0.6109 - acc: 0.749 - ETA: 2:40 - loss: 0.6108 - acc: 0.749 - ETA: 2:38 - loss: 0.6105 - acc: 0.748 - ETA: 2:36 - loss: 0.6098 - acc: 0.748 - ETA: 2:33 - loss: 0.6093 - acc: 0.747 - ETA: 2:30 - loss: 0.6072 - acc: 0.749 - ETA: 2:28 - loss: 0.6059 - acc: 0.749 - ETA: 2:26 - loss: 0.6055 - acc: 0.749 - ETA: 2:23 - loss: 0.6045 - acc: 0.749 - ETA: 2:21 - loss: 0.6036 - acc: 0.749 - ETA: 2:18 - loss: 0.6022 - acc: 0.750 - ETA: 2:16 - loss: 0.6024 - acc: 0.749 - ETA: 2:14 - loss: 0.6014 - acc: 0.749 - ETA: 2:12 - loss: 0.6006 - acc: 0.749 - ETA: 2:10 - loss: 0.5997 - acc: 0.750 - ETA: 2:08 - loss: 0.5992 - acc: 0.750 - ETA: 2:05 - loss: 0.5980 - acc: 0.750 - ETA: 2:03 - loss: 0.5960 - acc: 0.751 - ETA: 2:01 - loss: 0.5949 - acc: 0.752 - ETA: 1:59 - loss: 0.5946 - acc: 0.752 - ETA: 1:56 - loss: 0.5940 - acc: 0.752 - ETA: 1:54 - loss: 0.5931 - acc: 0.752 - ETA: 1:52 - loss: 0.5913 - acc: 0.753 - ETA: 1:50 - loss: 0.5906 - acc: 0.753 - ETA: 1:48 - loss: 0.5896 - acc: 0.754 - ETA: 1:46 - loss: 0.5894 - acc: 0.754 - ETA: 1:44 - loss: 0.5901 - acc: 0.753 - ETA: 1:41 - loss: 0.5891 - acc: 0.753 - ETA: 1:39 - loss: 0.5888 - acc: 0.753 - ETA: 1:37 - loss: 0.5885 - acc: 0.753 - ETA: 1:35 - loss: 0.5884 - acc: 0.753 - ETA: 1:33 - loss: 0.5893 - acc: 0.752 - ETA: 1:31 - loss: 0.5890 - acc: 0.752 - ETA: 1:29 - loss: 0.5881 - acc: 0.752 - ETA: 1:26 - loss: 0.5876 - acc: 0.753 - ETA: 1:24 - loss: 0.5869 - acc: 0.753 - ETA: 1:22 - loss: 0.5861 - acc: 0.753 - ETA: 1:20 - loss: 0.5856 - acc: 0.753 - ETA: 1:18 - loss: 0.5847 - acc: 0.754 - ETA: 1:16 - loss: 0.5849 - acc: 0.754 - ETA: 1:14 - loss: 0.5852 - acc: 0.753 - ETA: 1:12 - loss: 0.5847 - acc: 0.753 - ETA: 1:10 - loss: 0.5840 - acc: 0.753 - ETA: 1:08 - loss: 0.5843 - acc: 0.753 - ETA: 1:06 - loss: 0.5842 - acc: 0.753 - ETA: 1:04 - loss: 0.5828 - acc: 0.754 - ETA: 1:02 - loss: 0.5827 - acc: 0.754 - ETA: 59s - loss: 0.5822 - acc: 0.754 - ETA: 57s - loss: 0.5819 - acc: 0.75 - ETA: 56s - loss: 0.5808 - acc: 0.75 - ETA: 53s - loss: 0.5808 - acc: 0.75 - ETA: 51s - loss: 0.5802 - acc: 0.75 - ETA: 49s - loss: 0.5802 - acc: 0.75 - ETA: 47s - loss: 0.5802 - acc: 0.75 - ETA: 45s - loss: 0.5796 - acc: 0.75 - ETA: 43s - loss: 0.5800 - acc: 0.75 - ETA: 41s - loss: 0.5795 - acc: 0.75 - ETA: 39s - loss: 0.5795 - acc: 0.75 - ETA: 37s - loss: 0.5792 - acc: 0.75 - ETA: 35s - loss: 0.5792 - acc: 0.75 - ETA: 33s - loss: 0.5788 - acc: 0.75 - ETA: 31s - loss: 0.5781 - acc: 0.75 - ETA: 29s - loss: 0.5777 - acc: 0.75 - ETA: 27s - loss: 0.5776 - acc: 0.75 - ETA: 25s - loss: 0.5779 - acc: 0.75 - ETA: 23s - loss: 0.5776 - acc: 0.75 - ETA: 21s - loss: 0.5773 - acc: 0.75 - ETA: 19s - loss: 0.5769 - acc: 0.75 - ETA: 17s - loss: 0.5763 - acc: 0.75 - ETA: 15s - loss: 0.5758 - acc: 0.75 - ETA: 13s - loss: 0.5758 - acc: 0.75 - ETA: 11s - loss: 0.5761 - acc: 0.75 - ETA: 9s - loss: 0.5754 - acc: 0.7553 - ETA: 7s - loss: 0.5750 - acc: 0.755 - ETA: 5s - loss: 0.5746 - acc: 0.755 - ETA: 3s - loss: 0.5749 - acc: 0.755 - ETA: 1s - loss: 0.5748 - acc: 0.755 - 247s 15ms/step - loss: 0.5748 - acc: 0.7551 - val_loss: 0.5647 - val_acc: 0.7455\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - ETA: 3:44 - loss: 0.6048 - acc: 0.710 - ETA: 4:00 - loss: 0.5512 - acc: 0.757 - ETA: 3:58 - loss: 0.5365 - acc: 0.770 - ETA: 3:55 - loss: 0.5529 - acc: 0.755 - ETA: 4:01 - loss: 0.5488 - acc: 0.759 - ETA: 3:59 - loss: 0.5451 - acc: 0.763 - ETA: 3:52 - loss: 0.5562 - acc: 0.753 - ETA: 3:51 - loss: 0.5599 - acc: 0.750 - ETA: 3:49 - loss: 0.5608 - acc: 0.749 - ETA: 3:47 - loss: 0.5571 - acc: 0.752 - ETA: 3:43 - loss: 0.5497 - acc: 0.759 - ETA: 3:41 - loss: 0.5532 - acc: 0.755 - ETA: 3:43 - loss: 0.5496 - acc: 0.759 - ETA: 3:39 - loss: 0.5534 - acc: 0.755 - ETA: 3:35 - loss: 0.5486 - acc: 0.759 - ETA: 3:33 - loss: 0.5514 - acc: 0.757 - ETA: 3:29 - loss: 0.5472 - acc: 0.761 - ETA: 3:28 - loss: 0.5458 - acc: 0.762 - ETA: 3:25 - loss: 0.5437 - acc: 0.764 - ETA: 3:22 - loss: 0.5478 - acc: 0.760 - ETA: 3:19 - loss: 0.5485 - acc: 0.760 - ETA: 3:19 - loss: 0.5477 - acc: 0.760 - ETA: 3:17 - loss: 0.5471 - acc: 0.761 - ETA: 3:14 - loss: 0.5480 - acc: 0.760 - ETA: 3:11 - loss: 0.5480 - acc: 0.760 - ETA: 3:09 - loss: 0.5476 - acc: 0.760 - ETA: 3:07 - loss: 0.5512 - acc: 0.757 - ETA: 3:05 - loss: 0.5495 - acc: 0.758 - ETA: 3:03 - loss: 0.5504 - acc: 0.758 - ETA: 3:02 - loss: 0.5510 - acc: 0.757 - ETA: 3:00 - loss: 0.5517 - acc: 0.756 - ETA: 2:58 - loss: 0.5543 - acc: 0.754 - ETA: 2:56 - loss: 0.5559 - acc: 0.752 - ETA: 2:53 - loss: 0.5557 - acc: 0.753 - ETA: 2:52 - loss: 0.5558 - acc: 0.752 - ETA: 2:49 - loss: 0.5544 - acc: 0.754 - ETA: 2:47 - loss: 0.5536 - acc: 0.755 - ETA: 2:46 - loss: 0.5537 - acc: 0.754 - ETA: 2:44 - loss: 0.5531 - acc: 0.755 - ETA: 2:43 - loss: 0.5526 - acc: 0.755 - ETA: 2:41 - loss: 0.5520 - acc: 0.756 - ETA: 2:39 - loss: 0.5513 - acc: 0.757 - ETA: 2:38 - loss: 0.5532 - acc: 0.755 - ETA: 2:36 - loss: 0.5533 - acc: 0.755 - ETA: 2:34 - loss: 0.5525 - acc: 0.755 - ETA: 2:31 - loss: 0.5530 - acc: 0.755 - ETA: 2:30 - loss: 0.5524 - acc: 0.756 - ETA: 2:28 - loss: 0.5517 - acc: 0.756 - ETA: 2:25 - loss: 0.5518 - acc: 0.756 - ETA: 2:23 - loss: 0.5532 - acc: 0.755 - ETA: 2:21 - loss: 0.5540 - acc: 0.754 - ETA: 2:20 - loss: 0.5528 - acc: 0.755 - ETA: 2:18 - loss: 0.5513 - acc: 0.756 - ETA: 2:16 - loss: 0.5506 - acc: 0.757 - ETA: 2:14 - loss: 0.5513 - acc: 0.756 - ETA: 2:12 - loss: 0.5506 - acc: 0.757 - ETA: 2:10 - loss: 0.5502 - acc: 0.757 - ETA: 2:09 - loss: 0.5487 - acc: 0.758 - ETA: 2:07 - loss: 0.5491 - acc: 0.758 - ETA: 2:05 - loss: 0.5515 - acc: 0.756 - ETA: 2:03 - loss: 0.5513 - acc: 0.756 - ETA: 2:01 - loss: 0.5506 - acc: 0.757 - ETA: 1:59 - loss: 0.5503 - acc: 0.757 - ETA: 1:57 - loss: 0.5497 - acc: 0.757 - ETA: 1:55 - loss: 0.5508 - acc: 0.757 - ETA: 1:53 - loss: 0.5506 - acc: 0.757 - ETA: 1:51 - loss: 0.5511 - acc: 0.756 - ETA: 1:49 - loss: 0.5516 - acc: 0.756 - ETA: 1:47 - loss: 0.5522 - acc: 0.755 - ETA: 1:45 - loss: 0.5522 - acc: 0.755 - ETA: 1:43 - loss: 0.5528 - acc: 0.755 - ETA: 1:42 - loss: 0.5525 - acc: 0.755 - ETA: 1:40 - loss: 0.5524 - acc: 0.755 - ETA: 1:38 - loss: 0.5534 - acc: 0.754 - ETA: 1:36 - loss: 0.5532 - acc: 0.754 - ETA: 1:34 - loss: 0.5540 - acc: 0.753 - ETA: 1:32 - loss: 0.5531 - acc: 0.754 - ETA: 1:30 - loss: 0.5528 - acc: 0.754 - ETA: 1:28 - loss: 0.5534 - acc: 0.754 - ETA: 1:26 - loss: 0.5532 - acc: 0.754 - ETA: 1:24 - loss: 0.5528 - acc: 0.754 - ETA: 1:22 - loss: 0.5519 - acc: 0.755 - ETA: 1:20 - loss: 0.5527 - acc: 0.754 - ETA: 1:18 - loss: 0.5530 - acc: 0.754 - ETA: 1:16 - loss: 0.5523 - acc: 0.755 - ETA: 1:14 - loss: 0.5529 - acc: 0.754 - ETA: 1:13 - loss: 0.5532 - acc: 0.754 - ETA: 1:11 - loss: 0.5533 - acc: 0.754 - ETA: 1:09 - loss: 0.5525 - acc: 0.754 - ETA: 1:07 - loss: 0.5529 - acc: 0.754 - ETA: 1:05 - loss: 0.5527 - acc: 0.754 - ETA: 1:03 - loss: 0.5527 - acc: 0.754 - ETA: 1:01 - loss: 0.5529 - acc: 0.754 - ETA: 59s - loss: 0.5522 - acc: 0.754 - ETA: 57s - loss: 0.5527 - acc: 0.75 - ETA: 55s - loss: 0.5522 - acc: 0.75 - ETA: 54s - loss: 0.5524 - acc: 0.75 - ETA: 52s - loss: 0.5527 - acc: 0.75 - ETA: 50s - loss: 0.5527 - acc: 0.75 - ETA: 48s - loss: 0.5527 - acc: 0.75 - ETA: 46s - loss: 0.5526 - acc: 0.75 - ETA: 44s - loss: 0.5523 - acc: 0.75 - ETA: 42s - loss: 0.5518 - acc: 0.75 - ETA: 40s - loss: 0.5515 - acc: 0.75 - ETA: 38s - loss: 0.5504 - acc: 0.75 - ETA: 36s - loss: 0.5504 - acc: 0.75 - ETA: 34s - loss: 0.5500 - acc: 0.75 - ETA: 32s - loss: 0.5503 - acc: 0.75 - ETA: 30s - loss: 0.5503 - acc: 0.75 - ETA: 28s - loss: 0.5501 - acc: 0.75 - ETA: 26s - loss: 0.5493 - acc: 0.75 - ETA: 25s - loss: 0.5499 - acc: 0.75 - ETA: 23s - loss: 0.5490 - acc: 0.75 - ETA: 21s - loss: 0.5484 - acc: 0.75 - ETA: 19s - loss: 0.5484 - acc: 0.75 - ETA: 17s - loss: 0.5488 - acc: 0.75 - ETA: 15s - loss: 0.5495 - acc: 0.75 - ETA: 13s - loss: 0.5495 - acc: 0.75 - ETA: 11s - loss: 0.5498 - acc: 0.75 - ETA: 9s - loss: 0.5495 - acc: 0.7564 - ETA: 7s - loss: 0.5498 - acc: 0.756 - ETA: 5s - loss: 0.5494 - acc: 0.756 - ETA: 3s - loss: 0.5493 - acc: 0.756 - ETA: 1s - loss: 0.5500 - acc: 0.755 - 243s 15ms/step - loss: 0.5501 - acc: 0.7558 - val_loss: 0.5570 - val_acc: 0.7455\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - ETA: 4:02 - loss: 0.4725 - acc: 0.828 - ETA: 3:47 - loss: 0.5236 - acc: 0.777 - ETA: 4:01 - loss: 0.5254 - acc: 0.776 - ETA: 4:06 - loss: 0.5478 - acc: 0.753 - ETA: 4:02 - loss: 0.5352 - acc: 0.765 - ETA: 4:04 - loss: 0.5381 - acc: 0.763 - ETA: 3:58 - loss: 0.5420 - acc: 0.758 - ETA: 3:56 - loss: 0.5389 - acc: 0.761 - ETA: 3:52 - loss: 0.5364 - acc: 0.763 - ETA: 3:50 - loss: 0.5422 - acc: 0.758 - ETA: 3:49 - loss: 0.5470 - acc: 0.754 - ETA: 3:48 - loss: 0.5453 - acc: 0.755 - ETA: 3:43 - loss: 0.5444 - acc: 0.756 - ETA: 3:39 - loss: 0.5442 - acc: 0.756 - ETA: 3:38 - loss: 0.5428 - acc: 0.757 - ETA: 3:35 - loss: 0.5414 - acc: 0.759 - ETA: 3:33 - loss: 0.5417 - acc: 0.759 - ETA: 3:29 - loss: 0.5416 - acc: 0.759 - ETA: 3:28 - loss: 0.5438 - acc: 0.757 - ETA: 3:27 - loss: 0.5451 - acc: 0.755 - ETA: 3:25 - loss: 0.5458 - acc: 0.755 - ETA: 3:21 - loss: 0.5443 - acc: 0.756 - ETA: 3:20 - loss: 0.5470 - acc: 0.754 - ETA: 3:18 - loss: 0.5448 - acc: 0.755 - ETA: 3:15 - loss: 0.5452 - acc: 0.755 - ETA: 3:14 - loss: 0.5457 - acc: 0.754 - ETA: 3:12 - loss: 0.5446 - acc: 0.755 - ETA: 3:10 - loss: 0.5452 - acc: 0.755 - ETA: 3:10 - loss: 0.5450 - acc: 0.755 - ETA: 3:08 - loss: 0.5419 - acc: 0.758 - ETA: 3:06 - loss: 0.5398 - acc: 0.759 - ETA: 3:04 - loss: 0.5400 - acc: 0.759 - ETA: 3:01 - loss: 0.5400 - acc: 0.759 - ETA: 2:59 - loss: 0.5394 - acc: 0.759 - ETA: 2:57 - loss: 0.5378 - acc: 0.761 - ETA: 2:56 - loss: 0.5371 - acc: 0.761 - ETA: 2:54 - loss: 0.5379 - acc: 0.761 - ETA: 2:52 - loss: 0.5390 - acc: 0.760 - ETA: 2:50 - loss: 0.5405 - acc: 0.758 - ETA: 2:47 - loss: 0.5389 - acc: 0.760 - ETA: 2:46 - loss: 0.5388 - acc: 0.759 - ETA: 2:44 - loss: 0.5398 - acc: 0.759 - ETA: 2:41 - loss: 0.5396 - acc: 0.759 - ETA: 2:40 - loss: 0.5387 - acc: 0.759 - ETA: 2:38 - loss: 0.5393 - acc: 0.759 - ETA: 2:36 - loss: 0.5384 - acc: 0.760 - ETA: 2:34 - loss: 0.5394 - acc: 0.759 - ETA: 2:32 - loss: 0.5393 - acc: 0.759 - ETA: 2:30 - loss: 0.5386 - acc: 0.759 - ETA: 2:28 - loss: 0.5405 - acc: 0.758 - ETA: 2:25 - loss: 0.5409 - acc: 0.757 - ETA: 2:24 - loss: 0.5410 - acc: 0.757 - ETA: 2:22 - loss: 0.5397 - acc: 0.758 - ETA: 2:20 - loss: 0.5395 - acc: 0.759 - ETA: 2:18 - loss: 0.5392 - acc: 0.759 - ETA: 2:16 - loss: 0.5396 - acc: 0.758 - ETA: 2:14 - loss: 0.5399 - acc: 0.758 - ETA: 2:11 - loss: 0.5411 - acc: 0.757 - ETA: 2:09 - loss: 0.5409 - acc: 0.757 - ETA: 2:07 - loss: 0.5401 - acc: 0.758 - ETA: 2:06 - loss: 0.5404 - acc: 0.757 - ETA: 2:04 - loss: 0.5402 - acc: 0.757 - ETA: 2:02 - loss: 0.5404 - acc: 0.757 - ETA: 2:00 - loss: 0.5399 - acc: 0.758 - ETA: 1:57 - loss: 0.5415 - acc: 0.756 - ETA: 1:55 - loss: 0.5409 - acc: 0.757 - ETA: 1:53 - loss: 0.5403 - acc: 0.757 - ETA: 1:51 - loss: 0.5407 - acc: 0.757 - ETA: 1:50 - loss: 0.5410 - acc: 0.756 - ETA: 1:48 - loss: 0.5420 - acc: 0.755 - ETA: 1:46 - loss: 0.5428 - acc: 0.755 - ETA: 1:43 - loss: 0.5421 - acc: 0.755 - ETA: 1:42 - loss: 0.5423 - acc: 0.755 - ETA: 1:39 - loss: 0.5428 - acc: 0.755 - ETA: 1:37 - loss: 0.5438 - acc: 0.754 - ETA: 1:35 - loss: 0.5438 - acc: 0.754 - ETA: 1:34 - loss: 0.5433 - acc: 0.754 - ETA: 1:32 - loss: 0.5435 - acc: 0.754 - ETA: 1:30 - loss: 0.5439 - acc: 0.753 - ETA: 1:28 - loss: 0.5440 - acc: 0.753 - ETA: 1:26 - loss: 0.5432 - acc: 0.754 - ETA: 1:24 - loss: 0.5423 - acc: 0.755 - ETA: 1:22 - loss: 0.5431 - acc: 0.754 - ETA: 1:20 - loss: 0.5434 - acc: 0.754 - ETA: 1:18 - loss: 0.5438 - acc: 0.753 - ETA: 1:16 - loss: 0.5434 - acc: 0.753 - ETA: 1:14 - loss: 0.5427 - acc: 0.754 - ETA: 1:12 - loss: 0.5430 - acc: 0.754 - ETA: 1:10 - loss: 0.5429 - acc: 0.754 - ETA: 1:08 - loss: 0.5431 - acc: 0.753 - ETA: 1:06 - loss: 0.5427 - acc: 0.754 - ETA: 1:04 - loss: 0.5428 - acc: 0.754 - ETA: 1:02 - loss: 0.5426 - acc: 0.754 - ETA: 1:00 - loss: 0.5424 - acc: 0.754 - ETA: 58s - loss: 0.5424 - acc: 0.754 - ETA: 56s - loss: 0.5426 - acc: 0.75 - ETA: 55s - loss: 0.5421 - acc: 0.75 - ETA: 53s - loss: 0.5418 - acc: 0.75 - ETA: 50s - loss: 0.5412 - acc: 0.75 - ETA: 49s - loss: 0.5411 - acc: 0.75 - ETA: 47s - loss: 0.5412 - acc: 0.75 - ETA: 45s - loss: 0.5412 - acc: 0.75 - ETA: 43s - loss: 0.5401 - acc: 0.75 - ETA: 41s - loss: 0.5406 - acc: 0.75 - ETA: 39s - loss: 0.5403 - acc: 0.75 - ETA: 37s - loss: 0.5399 - acc: 0.75 - ETA: 35s - loss: 0.5399 - acc: 0.75 - ETA: 33s - loss: 0.5399 - acc: 0.75 - ETA: 31s - loss: 0.5393 - acc: 0.75 - ETA: 29s - loss: 0.5396 - acc: 0.75 - ETA: 27s - loss: 0.5394 - acc: 0.75 - ETA: 25s - loss: 0.5393 - acc: 0.75 - ETA: 23s - loss: 0.5390 - acc: 0.75 - ETA: 21s - loss: 0.5388 - acc: 0.75 - ETA: 19s - loss: 0.5388 - acc: 0.75 - ETA: 17s - loss: 0.5391 - acc: 0.75 - ETA: 15s - loss: 0.5388 - acc: 0.75 - ETA: 13s - loss: 0.5392 - acc: 0.75 - ETA: 11s - loss: 0.5389 - acc: 0.75 - ETA: 9s - loss: 0.5384 - acc: 0.7563 - ETA: 7s - loss: 0.5385 - acc: 0.756 - ETA: 5s - loss: 0.5385 - acc: 0.756 - ETA: 3s - loss: 0.5386 - acc: 0.755 - ETA: 1s - loss: 0.5389 - acc: 0.755 - 246s 15ms/step - loss: 0.5386 - acc: 0.7558 - val_loss: 0.5399 - val_acc: 0.7455\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - ETA: 4:53 - loss: 0.4516 - acc: 0.828 - ETA: 4:36 - loss: 0.4717 - acc: 0.808 - ETA: 4:07 - loss: 0.5053 - acc: 0.778 - ETA: 3:59 - loss: 0.5090 - acc: 0.773 - ETA: 3:49 - loss: 0.5122 - acc: 0.770 - ETA: 3:42 - loss: 0.5144 - acc: 0.766 - ETA: 3:42 - loss: 0.5262 - acc: 0.756 - ETA: 3:39 - loss: 0.5200 - acc: 0.761 - ETA: 3:42 - loss: 0.5182 - acc: 0.763 - ETA: 3:44 - loss: 0.5302 - acc: 0.753 - ETA: 3:40 - loss: 0.5290 - acc: 0.754 - ETA: 3:40 - loss: 0.5345 - acc: 0.748 - ETA: 3:36 - loss: 0.5283 - acc: 0.754 - ETA: 3:33 - loss: 0.5256 - acc: 0.756 - ETA: 3:31 - loss: 0.5266 - acc: 0.755 - ETA: 3:27 - loss: 0.5231 - acc: 0.758 - ETA: 3:25 - loss: 0.5252 - acc: 0.756 - ETA: 3:25 - loss: 0.5284 - acc: 0.753 - ETA: 3:23 - loss: 0.5239 - acc: 0.757 - ETA: 3:19 - loss: 0.5234 - acc: 0.756 - ETA: 3:17 - loss: 0.5213 - acc: 0.758 - ETA: 3:16 - loss: 0.5193 - acc: 0.760 - ETA: 3:13 - loss: 0.5239 - acc: 0.756 - ETA: 3:12 - loss: 0.5248 - acc: 0.754 - ETA: 3:11 - loss: 0.5252 - acc: 0.754 - ETA: 3:10 - loss: 0.5228 - acc: 0.756 - ETA: 3:09 - loss: 0.5204 - acc: 0.758 - ETA: 3:06 - loss: 0.5216 - acc: 0.757 - ETA: 3:04 - loss: 0.5202 - acc: 0.758 - ETA: 3:02 - loss: 0.5214 - acc: 0.757 - ETA: 3:00 - loss: 0.5197 - acc: 0.758 - ETA: 2:58 - loss: 0.5209 - acc: 0.757 - ETA: 2:55 - loss: 0.5212 - acc: 0.757 - ETA: 2:54 - loss: 0.5225 - acc: 0.756 - ETA: 2:53 - loss: 0.5231 - acc: 0.755 - ETA: 2:51 - loss: 0.5230 - acc: 0.755 - ETA: 2:49 - loss: 0.5242 - acc: 0.754 - ETA: 2:46 - loss: 0.5235 - acc: 0.754 - ETA: 2:44 - loss: 0.5226 - acc: 0.756 - ETA: 2:41 - loss: 0.5228 - acc: 0.755 - ETA: 2:39 - loss: 0.5215 - acc: 0.757 - ETA: 2:37 - loss: 0.5215 - acc: 0.756 - ETA: 2:36 - loss: 0.5205 - acc: 0.757 - ETA: 2:34 - loss: 0.5198 - acc: 0.758 - ETA: 2:32 - loss: 0.5200 - acc: 0.758 - ETA: 2:30 - loss: 0.5205 - acc: 0.757 - ETA: 2:27 - loss: 0.5201 - acc: 0.758 - ETA: 2:26 - loss: 0.5209 - acc: 0.757 - ETA: 2:23 - loss: 0.5210 - acc: 0.756 - ETA: 2:21 - loss: 0.5215 - acc: 0.756 - ETA: 2:20 - loss: 0.5207 - acc: 0.756 - ETA: 2:18 - loss: 0.5204 - acc: 0.757 - ETA: 2:16 - loss: 0.5196 - acc: 0.757 - ETA: 2:15 - loss: 0.5206 - acc: 0.756 - ETA: 2:13 - loss: 0.5208 - acc: 0.756 - ETA: 2:11 - loss: 0.5204 - acc: 0.756 - ETA: 2:10 - loss: 0.5200 - acc: 0.756 - ETA: 2:08 - loss: 0.5190 - acc: 0.757 - ETA: 2:08 - loss: 0.5182 - acc: 0.758 - ETA: 2:06 - loss: 0.5175 - acc: 0.758 - ETA: 2:04 - loss: 0.5188 - acc: 0.757 - ETA: 2:02 - loss: 0.5175 - acc: 0.758 - ETA: 2:00 - loss: 0.5166 - acc: 0.758 - ETA: 1:58 - loss: 0.5153 - acc: 0.759 - ETA: 1:56 - loss: 0.5146 - acc: 0.760 - ETA: 1:54 - loss: 0.5135 - acc: 0.760 - ETA: 1:53 - loss: 0.5140 - acc: 0.760 - ETA: 1:51 - loss: 0.5133 - acc: 0.760 - ETA: 1:48 - loss: 0.5132 - acc: 0.760 - ETA: 1:46 - loss: 0.5136 - acc: 0.760 - ETA: 1:45 - loss: 0.5145 - acc: 0.759 - ETA: 1:43 - loss: 0.5151 - acc: 0.758 - ETA: 1:41 - loss: 0.5147 - acc: 0.759 - ETA: 1:39 - loss: 0.5146 - acc: 0.758 - ETA: 1:37 - loss: 0.5142 - acc: 0.758 - ETA: 1:35 - loss: 0.5142 - acc: 0.758 - ETA: 1:33 - loss: 0.5143 - acc: 0.758 - ETA: 1:31 - loss: 0.5149 - acc: 0.757 - ETA: 1:29 - loss: 0.5144 - acc: 0.757 - ETA: 1:27 - loss: 0.5143 - acc: 0.757 - ETA: 1:25 - loss: 0.5137 - acc: 0.757 - ETA: 1:23 - loss: 0.5134 - acc: 0.757 - ETA: 1:21 - loss: 0.5130 - acc: 0.757 - ETA: 1:19 - loss: 0.5131 - acc: 0.757 - ETA: 1:17 - loss: 0.5135 - acc: 0.757 - ETA: 1:15 - loss: 0.5140 - acc: 0.756 - ETA: 1:13 - loss: 0.5141 - acc: 0.756 - ETA: 1:11 - loss: 0.5134 - acc: 0.756 - ETA: 1:09 - loss: 0.5136 - acc: 0.755 - ETA: 1:07 - loss: 0.5138 - acc: 0.755 - ETA: 1:05 - loss: 0.5132 - acc: 0.755 - ETA: 1:04 - loss: 0.5130 - acc: 0.755 - ETA: 1:02 - loss: 0.5126 - acc: 0.755 - ETA: 1:00 - loss: 0.5120 - acc: 0.756 - ETA: 58s - loss: 0.5120 - acc: 0.755 - ETA: 56s - loss: 0.5115 - acc: 0.75 - ETA: 54s - loss: 0.5110 - acc: 0.75 - ETA: 52s - loss: 0.5104 - acc: 0.75 - ETA: 50s - loss: 0.5096 - acc: 0.75 - ETA: 48s - loss: 0.5090 - acc: 0.75 - ETA: 46s - loss: 0.5086 - acc: 0.75 - ETA: 44s - loss: 0.5088 - acc: 0.75 - ETA: 42s - loss: 0.5085 - acc: 0.75 - ETA: 40s - loss: 0.5076 - acc: 0.75 - ETA: 38s - loss: 0.5075 - acc: 0.75 - ETA: 36s - loss: 0.5073 - acc: 0.75 - ETA: 34s - loss: 0.5076 - acc: 0.75 - ETA: 32s - loss: 0.5070 - acc: 0.75 - ETA: 30s - loss: 0.5063 - acc: 0.75 - ETA: 28s - loss: 0.5064 - acc: 0.75 - ETA: 27s - loss: 0.5064 - acc: 0.75 - ETA: 25s - loss: 0.5064 - acc: 0.75 - ETA: 23s - loss: 0.5059 - acc: 0.75 - ETA: 21s - loss: 0.5056 - acc: 0.75 - ETA: 19s - loss: 0.5057 - acc: 0.75 - ETA: 17s - loss: 0.5056 - acc: 0.75 - ETA: 15s - loss: 0.5056 - acc: 0.75 - ETA: 13s - loss: 0.5052 - acc: 0.75 - ETA: 11s - loss: 0.5058 - acc: 0.75 - ETA: 9s - loss: 0.5056 - acc: 0.7550 - ETA: 7s - loss: 0.5052 - acc: 0.755 - ETA: 5s - loss: 0.5048 - acc: 0.755 - ETA: 3s - loss: 0.5046 - acc: 0.755 - ETA: 1s - loss: 0.5043 - acc: 0.755 - 243s 15ms/step - loss: 0.5035 - acc: 0.7558 - val_loss: 0.4768 - val_acc: 0.7455\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - ETA: 4:33 - loss: 0.4187 - acc: 0.789 - ETA: 3:48 - loss: 0.4496 - acc: 0.773 - ETA: 3:45 - loss: 0.4592 - acc: 0.763 - ETA: 3:48 - loss: 0.4771 - acc: 0.746 - ETA: 3:45 - loss: 0.4727 - acc: 0.748 - ETA: 3:49 - loss: 0.4667 - acc: 0.753 - ETA: 3:48 - loss: 0.4700 - acc: 0.751 - ETA: 3:50 - loss: 0.4694 - acc: 0.751 - ETA: 3:46 - loss: 0.4672 - acc: 0.752 - ETA: 3:42 - loss: 0.4714 - acc: 0.746 - ETA: 3:40 - loss: 0.4693 - acc: 0.748 - ETA: 3:36 - loss: 0.4717 - acc: 0.744 - ETA: 3:33 - loss: 0.4722 - acc: 0.744 - ETA: 3:29 - loss: 0.4708 - acc: 0.746 - ETA: 3:27 - loss: 0.4738 - acc: 0.743 - ETA: 3:27 - loss: 0.4742 - acc: 0.743 - ETA: 3:26 - loss: 0.4764 - acc: 0.742 - ETA: 3:26 - loss: 0.4779 - acc: 0.740 - ETA: 3:24 - loss: 0.4733 - acc: 0.744 - ETA: 3:24 - loss: 0.4704 - acc: 0.747 - ETA: 3:22 - loss: 0.4677 - acc: 0.750 - ETA: 3:19 - loss: 0.4677 - acc: 0.749 - ETA: 3:18 - loss: 0.4665 - acc: 0.750 - ETA: 3:17 - loss: 0.4662 - acc: 0.749 - ETA: 3:14 - loss: 0.4632 - acc: 0.751 - ETA: 3:12 - loss: 0.4621 - acc: 0.751 - ETA: 3:10 - loss: 0.4615 - acc: 0.752 - ETA: 3:07 - loss: 0.4616 - acc: 0.752 - ETA: 3:05 - loss: 0.4633 - acc: 0.750 - ETA: 3:01 - loss: 0.4607 - acc: 0.753 - ETA: 3:00 - loss: 0.4582 - acc: 0.754 - ETA: 2:58 - loss: 0.4579 - acc: 0.754 - ETA: 2:57 - loss: 0.4578 - acc: 0.754 - ETA: 2:56 - loss: 0.4574 - acc: 0.755 - ETA: 2:54 - loss: 0.4579 - acc: 0.753 - ETA: 2:52 - loss: 0.4567 - acc: 0.755 - ETA: 2:50 - loss: 0.4557 - acc: 0.755 - ETA: 2:48 - loss: 0.4547 - acc: 0.756 - ETA: 2:46 - loss: 0.4541 - acc: 0.756 - ETA: 2:44 - loss: 0.4531 - acc: 0.757 - ETA: 2:42 - loss: 0.4529 - acc: 0.756 - ETA: 2:40 - loss: 0.4517 - acc: 0.757 - ETA: 2:37 - loss: 0.4524 - acc: 0.755 - ETA: 2:36 - loss: 0.4526 - acc: 0.755 - ETA: 2:34 - loss: 0.4514 - acc: 0.756 - ETA: 2:32 - loss: 0.4510 - acc: 0.756 - ETA: 2:30 - loss: 0.4498 - acc: 0.756 - ETA: 2:28 - loss: 0.4510 - acc: 0.755 - ETA: 2:27 - loss: 0.4487 - acc: 0.757 - ETA: 2:25 - loss: 0.4494 - acc: 0.756 - ETA: 2:23 - loss: 0.4491 - acc: 0.756 - ETA: 2:21 - loss: 0.4475 - acc: 0.757 - ETA: 2:20 - loss: 0.4468 - acc: 0.756 - ETA: 2:18 - loss: 0.4461 - acc: 0.757 - ETA: 2:16 - loss: 0.4455 - acc: 0.757 - ETA: 2:14 - loss: 0.4442 - acc: 0.758 - ETA: 2:12 - loss: 0.4436 - acc: 0.758 - ETA: 2:10 - loss: 0.4428 - acc: 0.759 - ETA: 2:08 - loss: 0.4424 - acc: 0.759 - ETA: 2:06 - loss: 0.4420 - acc: 0.759 - ETA: 2:04 - loss: 0.4408 - acc: 0.760 - ETA: 2:02 - loss: 0.4405 - acc: 0.760 - ETA: 1:59 - loss: 0.4407 - acc: 0.759 - ETA: 1:57 - loss: 0.4412 - acc: 0.759 - ETA: 1:55 - loss: 0.4414 - acc: 0.758 - ETA: 1:53 - loss: 0.4411 - acc: 0.758 - ETA: 1:51 - loss: 0.4408 - acc: 0.758 - ETA: 1:49 - loss: 0.4408 - acc: 0.758 - ETA: 1:47 - loss: 0.4401 - acc: 0.758 - ETA: 1:45 - loss: 0.4398 - acc: 0.758 - ETA: 1:43 - loss: 0.4396 - acc: 0.758 - ETA: 1:41 - loss: 0.4399 - acc: 0.757 - ETA: 1:39 - loss: 0.4393 - acc: 0.757 - ETA: 1:38 - loss: 0.4389 - acc: 0.757 - ETA: 1:36 - loss: 0.4397 - acc: 0.757 - ETA: 1:34 - loss: 0.4395 - acc: 0.757 - ETA: 1:32 - loss: 0.4394 - acc: 0.757 - ETA: 1:30 - loss: 0.4385 - acc: 0.758 - ETA: 1:28 - loss: 0.4382 - acc: 0.758 - ETA: 1:26 - loss: 0.4379 - acc: 0.758 - ETA: 1:24 - loss: 0.4380 - acc: 0.758 - ETA: 1:22 - loss: 0.4372 - acc: 0.758 - ETA: 1:20 - loss: 0.4367 - acc: 0.758 - ETA: 1:18 - loss: 0.4363 - acc: 0.758 - ETA: 1:16 - loss: 0.4354 - acc: 0.758 - ETA: 1:14 - loss: 0.4351 - acc: 0.759 - ETA: 1:12 - loss: 0.4345 - acc: 0.759 - ETA: 1:11 - loss: 0.4339 - acc: 0.760 - ETA: 1:09 - loss: 0.4338 - acc: 0.759 - ETA: 1:06 - loss: 0.4339 - acc: 0.759 - ETA: 1:05 - loss: 0.4342 - acc: 0.758 - ETA: 1:03 - loss: 0.4345 - acc: 0.758 - ETA: 1:01 - loss: 0.4340 - acc: 0.758 - ETA: 59s - loss: 0.4338 - acc: 0.758 - ETA: 57s - loss: 0.4337 - acc: 0.75 - ETA: 55s - loss: 0.4333 - acc: 0.75 - ETA: 53s - loss: 0.4334 - acc: 0.75 - ETA: 51s - loss: 0.4326 - acc: 0.75 - ETA: 49s - loss: 0.4318 - acc: 0.75 - ETA: 47s - loss: 0.4315 - acc: 0.75 - ETA: 45s - loss: 0.4312 - acc: 0.75 - ETA: 43s - loss: 0.4321 - acc: 0.75 - ETA: 41s - loss: 0.4327 - acc: 0.75 - ETA: 40s - loss: 0.4328 - acc: 0.75 - ETA: 38s - loss: 0.4327 - acc: 0.75 - ETA: 36s - loss: 0.4322 - acc: 0.75 - ETA: 34s - loss: 0.4322 - acc: 0.75 - ETA: 32s - loss: 0.4324 - acc: 0.75 - ETA: 30s - loss: 0.4320 - acc: 0.75 - ETA: 28s - loss: 0.4313 - acc: 0.75 - ETA: 26s - loss: 0.4312 - acc: 0.75 - ETA: 24s - loss: 0.4304 - acc: 0.76 - ETA: 22s - loss: 0.4298 - acc: 0.76 - ETA: 21s - loss: 0.4304 - acc: 0.75 - ETA: 19s - loss: 0.4303 - acc: 0.75 - ETA: 17s - loss: 0.4296 - acc: 0.76 - ETA: 15s - loss: 0.4297 - acc: 0.76 - ETA: 13s - loss: 0.4292 - acc: 0.76 - ETA: 11s - loss: 0.4291 - acc: 0.76 - ETA: 9s - loss: 0.4290 - acc: 0.7607 - ETA: 7s - loss: 0.4288 - acc: 0.760 - ETA: 5s - loss: 0.4286 - acc: 0.761 - ETA: 3s - loss: 0.4282 - acc: 0.761 - ETA: 1s - loss: 0.4278 - acc: 0.761 - 240s 15ms/step - loss: 0.4280 - acc: 0.7612 - val_loss: 0.4159 - val_acc: 0.7610\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Define number of epochs\n",
    "epochs = 5\n",
    "\n",
    "# Fit the model to the training data\n",
    "estimator = model.fit(X_train, y_train,\n",
    "                      validation_data=(X_test,y_test),\n",
    "                      epochs=epochs, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
