{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep NLP\n",
    "\n",
    "For this last class, we are going to try to build the neural network equivalents of the NLP models we built in week 4 of the course, and extend our understanding of how to build NLP models by utilizing contextual information from our corpuses.\n",
    "\n",
    "For the class, we will introduce two additional NLP libraries:\n",
    "\n",
    "- [spaCy](https://spacy.io/): Industrial-grade NLP library for building NLP pipelines over large corpora.\n",
    "- [gensim](https://radimrehurek.com/gensim/index.html): A library that allows us to generate our own word vector embeddings\n",
    "\n",
    "Use the following code to install the libraries we will need for this class:\n",
    "\n",
    "```\n",
    "conda install -c anaconda gensim\n",
    "conda install -c conda-forge spacy\n",
    "```\n",
    "\n",
    "You also need to download a `spaCy` model that we can use as part of our explorations. \n",
    "In order to do so, run the following from a terminal window:\n",
    "\n",
    "```\n",
    "python3 -m spacy download en\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Madhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import base64\n",
    "import string #use for punctuation removal\n",
    "import re\n",
    "from collections import Counter\n",
    "from time import time\n",
    "\n",
    "#nlp specific libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "#nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a smaller document set\n",
    "\n",
    "Using Deep Neural Networks is **incredibly computationally expensive.** So, we are not going to train these networks on all of the data, so I have generated a smaller dataset using only 20K yelp reviews.\n",
    "\n",
    "Our goal will be to simply compare a couple of different strategies for creating features from text, feeding them to either a multilayer perceptron (MLP) or a fairly deep convolutional neural net, and seeing the results. I'll explain each of these types of networks in turn.\n",
    "\n",
    "We will also attempt 2 distinct strategies on the data we have:\n",
    "\n",
    "1. Compute our own word vector representations on lightly preprocessed versions of the reviews.\n",
    "2. Use precomputed word vector representations on lightly preprocessed versions of the reviews.\n",
    "\n",
    "\n",
    "As we explore each strategy, we will get a feel for some of the limitations of both, and some overall strategies for  feature engineering our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8Gi91EkhG2_CJLk_WqjpEg</td>\n",
       "      <td>1</td>\n",
       "      <td>I found them through Craigslist. I did the 2 m...</td>\n",
       "      <td>um0ITBTHoohdozmFA6snlw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6zincCvQTb9BsuK-GHZ4cw</td>\n",
       "      <td>5</td>\n",
       "      <td>Great pizza, excellent delivery time &amp; if I ev...</td>\n",
       "      <td>tNcIhWEeAl607ENbeRSm0w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kzANsUYAZFnMsogB1RFE7A</td>\n",
       "      <td>5</td>\n",
       "      <td>Finding an incredible hairstylist is like find...</td>\n",
       "      <td>EJ7ZhRHsMWj8du77LX34gw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uBAHE6QOJlndILPIoeEpCQ</td>\n",
       "      <td>1</td>\n",
       "      <td>I attempted to use a coupon for the exact item...</td>\n",
       "      <td>RNVCLdKNddXp8v-y6Aq2Xg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yzmvsG3Vo-2_F3_0wHzJdA</td>\n",
       "      <td>1</td>\n",
       "      <td>Dammit! I had such high hopes for this place. ...</td>\n",
       "      <td>D5ETbJC0dptWR07sbTlqRg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  stars  \\\n",
       "0  8Gi91EkhG2_CJLk_WqjpEg      1   \n",
       "1  6zincCvQTb9BsuK-GHZ4cw      5   \n",
       "2  kzANsUYAZFnMsogB1RFE7A      5   \n",
       "3  uBAHE6QOJlndILPIoeEpCQ      1   \n",
       "4  yzmvsG3Vo-2_F3_0wHzJdA      1   \n",
       "\n",
       "                                                text                 user_id  \n",
       "0  I found them through Craigslist. I did the 2 m...  um0ITBTHoohdozmFA6snlw  \n",
       "1  Great pizza, excellent delivery time & if I ev...  tNcIhWEeAl607ENbeRSm0w  \n",
       "2  Finding an incredible hairstylist is like find...  EJ7ZhRHsMWj8du77LX34gw  \n",
       "3  I attempted to use a coupon for the exact item...  RNVCLdKNddXp8v-y6Aq2Xg  \n",
       "4  Dammit! I had such high hopes for this place. ...  D5ETbJC0dptWR07sbTlqRg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp = pd.read_csv(\"yelp_smaller_20k.csv\",index_col=0).reset_index(drop=True)\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    0.7537\n",
      "1    0.2463\n",
      "Name: stars, dtype: float64\n",
      "5    15074\n",
      "1     4926\n",
      "Name: stars, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(yelp.stars.value_counts()/yelp.shape[0])\n",
    "print(yelp.stars.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp[\"target\"] = (yelp.stars == 5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An overview of frequency embeddings\n",
    "\n",
    "Last week, we used `CountVectorizer` and `TFIDFVectorizer` to generate features from our texts. \n",
    "\n",
    "Both of the above feature engineering strategies are a kind of **embedding** of the text. \n",
    "\n",
    "**An embedding in mathematics is a projection of some data into some other fixed space.**\n",
    "\n",
    "In the case of the 2 above strategies, we are creating what are called **frequency-based embeddings** where we **embed** each individual variable-length text into a fixed-length vector (whose size is the number of distinct tokens we are using).\n",
    "\n",
    "So, we are completely ignoring the context (words surrounding each word) and simply creating a mapping of texts onto token frequency within that text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context-based Embeddings\n",
    "\n",
    "**However, there are other ways to generate embeddings!** \n",
    "\n",
    "One simple way to do this is to **create word co-occurrence matrices with some (usually fixed) context window**.\n",
    "\n",
    "What this means is you can simply count the frequency that a given word appears next to some other word, where *next to* is defined by the context window size (how many tokens away the context word is allowed to be in order to be considered part of the given word's context).\n",
    "\n",
    "E.g.:\n",
    "\n",
    "In the sentence **My name is Sergey and my home is in Brooklyn.** The co-occurrence value of the terms (Sergey,home) with a context window of 3 is 1, but would be 0 if the context window were 2.\n",
    "\n",
    "**Now, after we have computed all of our cooccurrence counts, we transform each of our texts into some aggregate of all of the cooccurrence pairs that exist for that text (usually either taking the sum or the average of all cooccurrence contexts).**\n",
    "\n",
    "However, there is a **huge** problem with this approach:\n",
    "- This yields massive cooccurrence matrix. It will be expensive to store (the number of distinct word/context pairs grows very quickly in the size of the vocabulary of the corpus).\n",
    "- This massive matrix is also incredibly sparse. Again, having massive, sparse spaces is very bad for machine learning.\n",
    "\n",
    "So, we need some way to shrink the size of this matrix. One way is to simply SVD or PCA it into some smaller size. This actually works reasonably well, but we can do even better..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context-based Embeddings: Neural-Network based Predictive Embeddings\n",
    "\n",
    "Now, let's go just a bit further. \n",
    "\n",
    "Instead of attempting to deterministically use the co-occurrence statistics of some text, which is a very high-dimensional representation of the corpus, we can try to create a model that predicts the most likely context given a specific word (or the most likely word given some context). \n",
    "\n",
    "The idea here is to learn some fixed-dimensional representation of the probability distribution of words given their contexts (**CBOW Word2Vec**) or contexts given a word (**skip-gram Word2Vec**).\n",
    "\n",
    "**In both cases, what we are actually doing is training a shallow (single hidden layer) neural network over our corpus of co-occurrence statistics and extracting the weights of the learned hidden layer after training.**\n",
    "\n",
    "So, the result of a word2vec training is generating a fixed-element distributed representation of each distinct word in the vocabulary defined over the corpus.\n",
    "\n",
    "Once we have such a representation, we can do interesting things with it like:\n",
    "\n",
    "- Find most similar words (since this is a vector, we simply compute the cosine similarity)\n",
    "- Word math (this is the famous $king - man + woman = queen$) example\n",
    "- Odd one out (given a list of words, which is least like the others)\n",
    "\n",
    "We can also then use these new word representations as inputs into ML pipelines (either simple models or deep neural nets, as we will see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW Word2Vec Architecture\n",
    "\n",
    "\n",
    "In the case where we are trying to predict the most likely word given some context.\n",
    "\n",
    "That is, we are trying to **maximize the probability of the target word by looking at the context.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single word context (what is the most likely word given the word \"really\"?):\n",
    "![word2vec](./images/word2vec_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Word Context, predicting most likely word (what is the most likely word in the context \"Today is the last ... of the session\"?):\n",
    "![word2vec2](./images/word2vec_network_2.png)\n",
    "\n",
    "\n",
    "In both of these cases, we will have a problem for rare words because the model is designed to predict the most probable word. Whatever this word is, it will be smoothed over a lot of examples with more frequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram Word2Vec Architecture\n",
    "\n",
    "In this case, we can turn the problem on its head, predicting the most likely context given some word.\n",
    "![word2vec3](./images/word2vec_network_3.png)\n",
    "\n",
    "Because the skip-gram model is designed to predict the context (rather than the most likely word), rare words do not compete with more frequent words.\n",
    "\n",
    "\n",
    "Ultimately, both of these models have tradeoffs:\n",
    "- **CBOW**: Very fast training (much faster than skip-gram), very good representation of the most frequent words in your vocabulary.\n",
    "- **Skip-gram**: Slower to train, but preferred with a small amount of the training data, represents rare words or phrases well.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Embedding Architectures\n",
    "\n",
    "- Doc2Vec\n",
    "- Sense2Vec\n",
    "- GloVe - word2vec ultimately loses the statistical properties of corpus (cooccurrence statistics are not kept, since word2vec turns this into a prediction problem). GloVe factorizes the cooccurrence matrix in an interesting way (sort of like PCA/SVD) and can sometimes lead to better embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your own word2vec model on yelp data\n",
    "\n",
    "The process will be as follows:\n",
    "- We need to generate lists of sentences across the entire corpus. It doesnt matter what document each sentence comes from. \n",
    "- We will lemmatize individual words, and split entire documents into sentences as below.\n",
    "- Following preprocessing, we train word2vec on the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to preprocess text for a word2vec model\n",
    "def cleanup_text_word2vec(docs, logging=False):\n",
    "    sentences = []\n",
    "    counter = 1\n",
    "    for doc in nlp.pipe(docs,n_threads=4,disable=[\"tagger\"]):\n",
    "        if counter % 1000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents\" % (counter, len(docs)))\n",
    "        # Grab lemmatized form of words and make lowercase\n",
    "        doc = \" \".join([tok.lemma_.lower() for tok in doc])\n",
    "        # Split into sentences based on punctuation - assuming a period, question mark, exclamation point, and apostroper\n",
    "        # dictate the end of a given sentence.\n",
    "        doc = re.split(\"[\\.?!;] \", doc)\n",
    "        # Remove commas, periods, and other punctuation (mostly commas) from each sentence\n",
    "        doc = [re.sub(\"[\\.,;:!?]\", \"\", sent) for sent in doc]\n",
    "        # Split each sentence into distinct words\n",
    "        doc = [sent.split() for sent in doc]\n",
    "        sentences += doc\n",
    "        counter += 1\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 out of 20000 documents\n",
      "Processed 2000 out of 20000 documents\n",
      "Processed 3000 out of 20000 documents\n",
      "Processed 4000 out of 20000 documents\n",
      "Processed 5000 out of 20000 documents\n",
      "Processed 6000 out of 20000 documents\n",
      "Processed 7000 out of 20000 documents\n",
      "Processed 8000 out of 20000 documents\n",
      "Processed 9000 out of 20000 documents\n",
      "Processed 10000 out of 20000 documents\n",
      "Processed 11000 out of 20000 documents\n",
      "Processed 12000 out of 20000 documents\n",
      "Processed 13000 out of 20000 documents\n",
      "Processed 14000 out of 20000 documents\n",
      "Processed 15000 out of 20000 documents\n",
      "Processed 16000 out of 20000 documents\n",
      "Processed 17000 out of 20000 documents\n",
      "Processed 18000 out of 20000 documents\n",
      "Processed 19000 out of 20000 documents\n",
      "Processed 20000 out of 20000 documents\n"
     ]
    }
   ],
   "source": [
    "yelp_cleaned_word2vec = cleanup_text_word2vec(yelp.text, logging=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we just have a list of lists, where each sublist is a sentence, and each token in that sentence is a word. We have now lost all of the information regarding what reviews each sentence originally belonged to (we will need this information later).\n",
    "\n",
    "\n",
    "Here are the first 5 processed sentences in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'find', 'them', 'through', 'craigslist'], ['i', 'do', 'the', '2', 'maid', 'for', '68', 'dollar', 'for', 'two', 'hour'], ['first', 'thing', 'be', 'they', 'be', 'pretty', 'late', 'about', '30', 'minute', 'late'], ['but', 'i', 'be', 'understand', 'a', 'i', 'do', 'live', 'in', 'a', 'far', 'fling', 'area', 'of', 'the', 'valley'], ['they', 'do', 'speak', 'very', 'little', 'english', 'and', 'i', 'speak', 'a', 'little', 'spanish', 'but', 'we', 'be', 'able', 'to', 'communicate', 'good', 'enough']]\n"
     ]
    }
   ],
   "source": [
    "print(yelp_cleaned_word2vec[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using [gensim](https://radimrehurek.com/gensim/models/word2vec.html) to build word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Word2Vec model created.\n",
      "13855 unique words represented by 300 dimensional vectors\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "text_dim = 300\n",
    "print(\"Training Word2Vec model...\")\n",
    "wordvec_model_300 = Word2Vec(yelp_cleaned_word2vec, size=text_dim, window=5, min_count=3, workers=4, sg=1)\n",
    "print(\"Word2Vec model created.\")\n",
    "print(\"%d unique words represented by %d dimensional vectors\" % (len(wordvec_model_300.wv.vocab), text_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test to see if the model is making reasonable embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to breakfast:\n",
      " [('brunch', 0.7401005029678345), ('lunch', 0.7071179151535034), ('continental', 0.6997714042663574), ('takeout', 0.6910825371742249), ('hangover', 0.6854385137557983), ('bagel', 0.6732635498046875), ('burrito', 0.6707483530044556), ('nachos', 0.6685673594474792), ('omelet', 0.663787841796875), ('cheeseburger', 0.6620101928710938)]\n",
      "\n",
      "Words most similar to breakfast minus lunch:\n",
      " [('chocolate', 1.1713966131210327), ('cream', 1.168455719947815), ('apple', 1.1631273031234741), ('vanilla', 1.160402536392212), ('donut', 1.1572304964065552), ('ice', 1.155745506286621), ('banana', 1.1552790403366089), ('custard', 1.1518254280090332), ('muffin', 1.1508272886276245), ('bake', 1.1500821113586426)]\n",
      "\n",
      "Which of the words breakfast stupid dinner lunch don't match:  stupid\n",
      "\n",
      "How related is breakfast to brunch:  0.7401005582820208\n",
      "\n",
      "How related is breakfast to dinner:  0.6201278397580494\n"
     ]
    }
   ],
   "source": [
    "print(\"Words most similar to breakfast:\\n\",wordvec_model_300.wv.most_similar(positive=['breakfast']))\n",
    "print()\n",
    "print(\"Words most similar to breakfast minus lunch:\\n\",wordvec_model_300.wv.most_similar_cosmul(positive=['breakfast'], negative=[\"lunch\"]))\n",
    "print()\n",
    "print(\"Which of the words breakfast stupid dinner lunch don\\'t match: \",wordvec_model_300.wv.doesnt_match(\"breakfast stupid dinner lunch\".split()))\n",
    "print()\n",
    "print(\"How related is breakfast to brunch: \",wordvec_model_300.wv.similarity('breakfast', 'brunch'))\n",
    "print()\n",
    "print(\"How related is breakfast to dinner: \",wordvec_model_300.wv.similarity('breakfast', 'dinner'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this looks good! Looks like our learned word embeddings pass the initial smell test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Alter the size of the embedding dimension. How does this change word similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Word2Vec model created.\n",
      "13855 unique words represented by 100 dimensional vectors\n"
     ]
    }
   ],
   "source": [
    "text_dim = 100\n",
    "print(\"Training Word2Vec model...\")\n",
    "wordvec_model = Word2Vec(yelp_cleaned_word2vec, size=text_dim, window=5, min_count=3, workers=4, sg=1)\n",
    "print(\"Word2Vec model created.\")\n",
    "print(\"%d unique words represented by %d dimensional vectors\" % (len(wordvec_model.wv.vocab), text_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to breakfast:\n",
      " [('lunch', 0.7887130975723267), ('brunch', 0.7867841124534607), ('hangover', 0.7359310388565063), ('baklava', 0.7228267788887024), ('bagel', 0.7200120687484741), ('continental', 0.7187748551368713), ('omelet', 0.7150955200195312), ('shawarma', 0.7141053676605225), ('cheesesteak', 0.7123523354530334), ('gyros', 0.7100532054901123)]\n",
      "\n",
      "Words most similar to breakfast minus lunch:\n",
      " [('cream', 1.201401948928833), ('chocolate', 1.1742335557937622), ('muffin', 1.173460602760315), ('ice', 1.1727256774902344), ('vanilla', 1.1705455780029297), ('apple', 1.169739842414856), ('donut', 1.169620156288147), ('custard', 1.167542815208435), ('creme', 1.162365436553955), ('divine', 1.1607450246810913)]\n",
      "\n",
      "Which of the words breakfast stupid dinner lunch don't match:  stupid\n",
      "\n",
      "How related is breakfast to brunch:  0.7867841170428731\n",
      "\n",
      "How related is breakfast to dinner:  0.687764313389204\n"
     ]
    }
   ],
   "source": [
    "print(\"Words most similar to breakfast:\\n\",wordvec_model.wv.most_similar(positive=['breakfast']))\n",
    "print()\n",
    "print(\"Words most similar to breakfast minus lunch:\\n\",wordvec_model.wv.most_similar_cosmul(positive=['breakfast'], negative=[\"lunch\"]))\n",
    "print()\n",
    "print(\"Which of the words breakfast stupid dinner lunch don\\'t match: \",wordvec_model.wv.doesnt_match(\"breakfast stupid dinner lunch\".split()))\n",
    "print()\n",
    "print(\"How related is breakfast to brunch: \",wordvec_model.wv.similarity('breakfast', 'brunch'))\n",
    "print()\n",
    "print(\"How related is breakfast to dinner: \",wordvec_model.wv.similarity('breakfast', 'dinner'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we now use these embeddings for our original purpose (document classification)?\n",
    "\n",
    "One way is to reprocess our text into strings of words, generate each word's embedding, and average across all embeddings.\n",
    "\n",
    "So, let's create a function that given some document (yelp review), computes the word-embedding average across the entire document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define function to create word vectors given a cleaned piece of text.\n",
    "def create_average_vec(doc):\n",
    "    average = np.zeros((text_dim,), dtype='float32')\n",
    "    num_words = 0.\n",
    "    doc = doc.split(\" \")\n",
    "    for word in doc:\n",
    "        if word in wordvec_model_300.wv.vocab:\n",
    "            average = np.add(average, wordvec_model_300[word])\n",
    "            num_words += 1.\n",
    "    if num_words != 0.:\n",
    "        average = np.divide(average, num_words)\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(0.0) == float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, now we have a problem. Our original data we trained word2vec on was a list of lists where we lost the indices of each sentence. What we will need to do is reprocess our original yelp review dataset in such a way that we can actually keep all of the indices of the original documents. That is, strip all of the sentence information, but keep the tokens from each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define function to cleanup text by removing stopwords, and punctuation\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def cleanup_text(docs, logging=True):\n",
    "    proc_docs = []\n",
    "    counter = 1\n",
    "    for doc in nlp.pipe(docs, n_threads=4,disable=[\"tagger\"]):\n",
    "        if counter % 200 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "        counter += 1\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc] #lemmatize, remove pronouns #if tok.lemma_ != '-PRON-'\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords.words() and tok not in punctuations] #remove stopwords, punctuation\n",
    "        tokens = ' '.join(tokens)\n",
    "        proc_docs.append(tokens)\n",
    "    return proc_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200 out of 20000 documents.\n",
      "Processed 400 out of 20000 documents.\n",
      "Processed 600 out of 20000 documents.\n",
      "Processed 800 out of 20000 documents.\n",
      "Processed 1000 out of 20000 documents.\n",
      "Processed 1200 out of 20000 documents.\n",
      "Processed 1400 out of 20000 documents.\n",
      "Processed 1600 out of 20000 documents.\n",
      "Processed 1800 out of 20000 documents.\n",
      "Processed 2000 out of 20000 documents.\n",
      "Processed 2200 out of 20000 documents.\n",
      "Processed 2400 out of 20000 documents.\n",
      "Processed 2600 out of 20000 documents.\n",
      "Processed 2800 out of 20000 documents.\n",
      "Processed 3000 out of 20000 documents.\n",
      "Processed 3200 out of 20000 documents.\n",
      "Processed 3400 out of 20000 documents.\n",
      "Processed 3600 out of 20000 documents.\n",
      "Processed 3800 out of 20000 documents.\n",
      "Processed 4000 out of 20000 documents.\n",
      "Processed 4200 out of 20000 documents.\n",
      "Processed 4400 out of 20000 documents.\n",
      "Processed 4600 out of 20000 documents.\n",
      "Processed 4800 out of 20000 documents.\n",
      "Processed 5000 out of 20000 documents.\n",
      "Processed 5200 out of 20000 documents.\n",
      "Processed 5400 out of 20000 documents.\n",
      "Processed 5600 out of 20000 documents.\n",
      "Processed 5800 out of 20000 documents.\n",
      "Processed 6000 out of 20000 documents.\n",
      "Processed 6200 out of 20000 documents.\n",
      "Processed 6400 out of 20000 documents.\n",
      "Processed 6600 out of 20000 documents.\n",
      "Processed 6800 out of 20000 documents.\n",
      "Processed 7000 out of 20000 documents.\n",
      "Processed 7200 out of 20000 documents.\n",
      "Processed 7400 out of 20000 documents.\n",
      "Processed 7600 out of 20000 documents.\n",
      "Processed 7800 out of 20000 documents.\n",
      "Processed 8000 out of 20000 documents.\n",
      "Processed 8200 out of 20000 documents.\n",
      "Processed 8400 out of 20000 documents.\n",
      "Processed 8600 out of 20000 documents.\n",
      "Processed 8800 out of 20000 documents.\n",
      "Processed 9000 out of 20000 documents.\n",
      "Processed 9200 out of 20000 documents.\n",
      "Processed 9400 out of 20000 documents.\n",
      "Processed 9600 out of 20000 documents.\n",
      "Processed 9800 out of 20000 documents.\n",
      "Processed 10000 out of 20000 documents.\n",
      "Processed 10200 out of 20000 documents.\n",
      "Processed 10400 out of 20000 documents.\n",
      "Processed 10600 out of 20000 documents.\n",
      "Processed 10800 out of 20000 documents.\n",
      "Processed 11000 out of 20000 documents.\n",
      "Processed 11200 out of 20000 documents.\n",
      "Processed 11400 out of 20000 documents.\n",
      "Processed 11600 out of 20000 documents.\n",
      "Processed 11800 out of 20000 documents.\n",
      "Processed 12000 out of 20000 documents.\n",
      "Processed 12200 out of 20000 documents.\n",
      "Processed 12400 out of 20000 documents.\n",
      "Processed 12600 out of 20000 documents.\n",
      "Processed 12800 out of 20000 documents.\n",
      "Processed 13000 out of 20000 documents.\n",
      "Processed 13200 out of 20000 documents.\n",
      "Processed 13400 out of 20000 documents.\n",
      "Processed 13600 out of 20000 documents.\n",
      "Processed 13800 out of 20000 documents.\n",
      "Processed 14000 out of 20000 documents.\n",
      "Processed 14200 out of 20000 documents.\n",
      "Processed 14400 out of 20000 documents.\n",
      "Processed 14600 out of 20000 documents.\n",
      "Processed 14800 out of 20000 documents.\n",
      "Processed 15000 out of 20000 documents.\n",
      "Processed 15200 out of 20000 documents.\n",
      "Processed 15400 out of 20000 documents.\n",
      "Processed 15600 out of 20000 documents.\n",
      "Processed 15800 out of 20000 documents.\n",
      "Processed 16000 out of 20000 documents.\n",
      "Processed 16200 out of 20000 documents.\n",
      "Processed 16400 out of 20000 documents.\n",
      "Processed 16600 out of 20000 documents.\n",
      "Processed 16800 out of 20000 documents.\n",
      "Processed 17000 out of 20000 documents.\n",
      "Processed 17200 out of 20000 documents.\n",
      "Processed 17400 out of 20000 documents.\n",
      "Processed 17600 out of 20000 documents.\n",
      "Processed 17800 out of 20000 documents.\n",
      "Processed 18000 out of 20000 documents.\n",
      "Processed 18200 out of 20000 documents.\n",
      "Processed 18400 out of 20000 documents.\n",
      "Processed 18600 out of 20000 documents.\n",
      "Processed 18800 out of 20000 documents.\n",
      "Processed 19000 out of 20000 documents.\n",
      "Processed 19200 out of 20000 documents.\n",
      "Processed 19400 out of 20000 documents.\n",
      "Processed 19600 out of 20000 documents.\n",
      "Processed 19800 out of 20000 documents.\n",
      "Processed 20000 out of 20000 documents.\n"
     ]
    }
   ],
   "source": [
    "cleaned_yelp_text = cleanup_text(yelp.text, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>find craigslist 2 maid 68 dollar two hour firs...</td>\n",
       "      <td>I found them through Craigslist. I did the 2 m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great pizza excellent delivery time ever back ...</td>\n",
       "      <td>Great pizza, excellent delivery time &amp; if I ev...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finding incredible hairstylist like find unico...</td>\n",
       "      <td>Finding an incredible hairstylist is like find...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>attempt use coupon exact item state picture ma...</td>\n",
       "      <td>I attempted to use a coupon for the exact item...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dammit high hope place -pron- eat month finall...</td>\n",
       "      <td>Dammit! I had such high hopes for this place. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  \\\n",
       "0  find craigslist 2 maid 68 dollar two hour firs...   \n",
       "1  great pizza excellent delivery time ever back ...   \n",
       "2  finding incredible hairstylist like find unico...   \n",
       "3  attempt use coupon exact item state picture ma...   \n",
       "4  dammit high hope place -pron- eat month finall...   \n",
       "\n",
       "                                                text  stars  target  \n",
       "0  I found them through Craigslist. I did the 2 m...      1       0  \n",
       "1  Great pizza, excellent delivery time & if I ev...      5       1  \n",
       "2  Finding an incredible hairstylist is like find...      5       1  \n",
       "3  I attempted to use a coupon for the exact item...      1       0  \n",
       "4  Dammit! I had such high hopes for this place. ...      1       0  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up all text DO NOT RUN THIS AS IT TAKES SOME TIME\n",
    "\n",
    "#yelp_smaller_clean = pd.Series(cleanup_text(yelp.text,logging=True))\n",
    "\n",
    "# LOAD IN THE CLEANED DATA INSTEAD :)\n",
    "yelp_smaller_clean = pd.read_csv(\"./yelp_cleaned_word2vec.csv\")\n",
    "yelp_smaller_clean.head()\n",
    "yelp_smaller_clean.cleaned_text.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final word vector shape: (20000, 300)\n"
     ]
    }
   ],
   "source": [
    "final_cleaned_vec = np.zeros((yelp_smaller_clean.shape[0], text_dim), dtype=\"float32\")  # 20000 x 300\n",
    "for i in range(yelp_smaller_clean.shape[0]):\n",
    "    final_cleaned_vec[i] = create_average_vec(yelp_smaller_clean.cleaned_text.values[i])\n",
    "\n",
    "print(\"Final word vector shape:\", final_cleaned_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example final vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.60599495e-02, -9.28331073e-03, -7.08896071e-02,  9.19171795e-02,\n",
       "       -1.51448678e-02,  2.50284635e-02, -4.37611751e-02, -2.09709466e-01,\n",
       "       -9.79925040e-03, -3.84979583e-02, -3.33492197e-02,  1.14552528e-01,\n",
       "       -9.56085026e-02, -9.21426564e-02, -8.17813072e-03, -7.64578879e-02,\n",
       "       -8.61215591e-02, -2.57790629e-02, -9.30847134e-04,  9.52796042e-02,\n",
       "       -9.62017030e-02, -1.49020776e-01,  3.18391211e-02,  6.60932809e-02,\n",
       "        7.78950527e-02,  7.99593404e-02,  1.00615986e-01,  9.32607520e-03,\n",
       "        5.69754541e-02, -1.33204699e-01,  1.00597061e-01,  2.56251507e-02,\n",
       "        8.77018198e-02,  7.47963861e-02,  2.07077786e-02, -1.38422996e-01,\n",
       "       -1.30761519e-01,  3.90914716e-02, -6.33014888e-02,  1.71119139e-01,\n",
       "        1.35665447e-01,  1.39350057e-01, -9.13011804e-02, -3.83911058e-02,\n",
       "       -3.56462896e-02, -4.05536927e-02,  1.81750163e-01, -9.79515016e-02,\n",
       "        7.81220421e-02, -4.41372804e-02, -8.22386816e-02, -1.08498238e-01,\n",
       "        1.51756117e-02,  4.68011051e-02, -9.62145627e-02,  4.30619456e-02,\n",
       "        1.05943955e-01, -1.17173698e-02, -3.17542739e-02,  1.29130244e-01,\n",
       "       -4.96419296e-02,  1.24624923e-01,  1.70932040e-02,  6.07544743e-02,\n",
       "       -1.49599407e-02, -1.51160257e-02, -1.52018238e-02, -7.11011887e-02,\n",
       "       -4.66579162e-02,  6.51257345e-03, -1.10458386e-04,  7.96851143e-02,\n",
       "       -5.23151793e-02, -7.41293952e-02, -6.47630990e-02,  2.03013241e-01,\n",
       "       -2.61638701e-01, -4.84812111e-02,  2.16350466e-01, -2.27811679e-01,\n",
       "        7.05847889e-02, -4.65386407e-03, -5.09190485e-02, -1.82547346e-02,\n",
       "       -1.14431800e-02, -1.04383372e-01,  8.64966288e-02,  1.66316435e-01,\n",
       "        1.81559816e-01,  3.22847739e-02, -1.07879706e-01, -6.25462160e-02,\n",
       "       -8.49853233e-02, -1.09407762e-02,  2.76070207e-01,  1.46004230e-01,\n",
       "        2.44530756e-02,  2.21640185e-01, -1.40009820e-01, -3.54143195e-02,\n",
       "       -1.86273381e-01, -2.20169146e-02, -4.04459126e-02,  2.23877952e-02,\n",
       "       -1.01205766e-01, -9.04156864e-02,  3.25776637e-02,  1.57753840e-01,\n",
       "        1.18018657e-01,  6.52007982e-02, -1.97748408e-01,  4.30109464e-02,\n",
       "        1.05949819e-01, -6.07196130e-02,  3.69280949e-02, -1.06016278e-01,\n",
       "       -4.13284563e-02, -1.18119173e-01, -2.46574301e-02,  4.16315012e-02,\n",
       "       -6.64588585e-02, -4.08246815e-02,  2.29492709e-01, -1.76315770e-01,\n",
       "       -9.01283044e-03,  7.49560520e-02, -2.41543576e-02,  6.54012859e-02,\n",
       "        7.16524124e-02,  9.86292660e-02, -1.07583530e-01,  8.84183943e-02,\n",
       "       -2.07895443e-01, -8.64621773e-02, -2.20914930e-02,  9.39425230e-02,\n",
       "        7.86440447e-02,  1.00966588e-01, -2.94388887e-02,  2.37418078e-02,\n",
       "       -1.50647879e-01, -2.70545837e-02, -2.38356534e-02,  9.14993063e-02,\n",
       "        4.12041917e-02,  5.22997640e-02,  4.79854196e-02,  6.43997565e-02,\n",
       "       -2.98318127e-03,  1.07219510e-01, -1.06121056e-01, -7.90809244e-02,\n",
       "       -3.34080718e-02,  6.94340616e-02, -4.95160557e-02,  1.40380800e-01,\n",
       "       -3.59898396e-02,  5.47116250e-02, -1.14536189e-01,  2.05047838e-02,\n",
       "        3.19169052e-02, -1.08949482e-01,  1.46180958e-01, -1.86183870e-01,\n",
       "       -5.71180731e-02, -8.14769715e-02,  3.04500926e-02, -1.39596611e-01,\n",
       "       -1.23615988e-01,  2.58205682e-02, -2.73646384e-01, -3.74178179e-02,\n",
       "        1.14940226e-01, -2.45547332e-02, -2.22635016e-01,  3.30448225e-02,\n",
       "        1.03532203e-01, -1.14399709e-01,  2.87021953e-03, -1.87568992e-01,\n",
       "        1.21775597e-01, -8.51028860e-02, -1.38425932e-03, -9.61787254e-02,\n",
       "       -1.38792947e-01, -5.30409962e-02,  7.58170187e-02,  1.75869111e-02,\n",
       "       -1.70202449e-01, -2.77320176e-01, -4.85866852e-02, -2.80523635e-02,\n",
       "       -6.14861399e-02, -1.78121597e-01,  1.13320008e-01,  7.78021067e-02,\n",
       "        3.34050320e-02, -6.76300377e-02,  1.30935544e-02, -3.28047536e-02,\n",
       "        1.90112159e-01,  9.70236063e-02,  3.19027342e-02,  2.24116407e-02,\n",
       "       -5.02995662e-02,  1.06815264e-01, -5.06610237e-03, -3.78324203e-02,\n",
       "        2.53517553e-02, -3.55267487e-02,  7.46081397e-02, -3.51703800e-02,\n",
       "        1.21611054e-04, -2.51765028e-02, -1.10542819e-01, -1.80199564e-01,\n",
       "        2.03509480e-02, -1.52098566e-01,  1.71339944e-01,  1.84647992e-01,\n",
       "       -1.76255122e-01, -1.29928976e-01,  2.80459728e-02,  1.14647470e-01,\n",
       "        7.36115128e-02, -5.04026413e-02, -2.08668560e-01,  1.07289841e-02,\n",
       "       -8.55996162e-02,  9.81913432e-02,  1.24268167e-01, -1.16254129e-01,\n",
       "       -4.03250232e-02, -1.90585762e-01,  8.03655460e-02,  1.09098956e-01,\n",
       "       -6.15583882e-02,  5.06525449e-02,  1.40238851e-01,  2.17169628e-01,\n",
       "       -3.39818783e-02, -7.14446697e-03,  1.53672665e-01, -4.35983352e-02,\n",
       "       -1.73316717e-01,  1.38447881e-01, -1.74192742e-01, -1.17422402e-01,\n",
       "        9.67505015e-03,  4.50554527e-02,  1.40260071e-01, -5.62810805e-03,\n",
       "        2.45792165e-01, -7.65759796e-02, -8.14367533e-02,  1.27174586e-01,\n",
       "        1.91749819e-02, -1.22393973e-01, -5.05148396e-02, -9.22272261e-03,\n",
       "       -5.98588921e-02, -1.08702384e-01,  1.54839277e-01, -7.09320381e-02,\n",
       "        1.14214398e-01, -9.12387669e-02, -1.96315292e-02, -1.12927124e-01,\n",
       "        1.25316560e-01,  1.96432099e-01,  1.42566804e-02, -1.56737026e-02,\n",
       "       -9.37512666e-02,  1.61942646e-01,  1.03020869e-01,  1.11079507e-01,\n",
       "        7.60043487e-02, -1.47333637e-01, -6.15777969e-02,  7.88682923e-02,\n",
       "        1.52012780e-01, -1.33576706e-01, -1.19266063e-01,  5.54377399e-02,\n",
       "        9.99198779e-02, -3.48601304e-02, -3.38995419e-02, -6.69695437e-02,\n",
       "        9.98604596e-02,  5.20986505e-02, -7.20314160e-02, -1.65626094e-01,\n",
       "        1.38039976e-01, -1.08623490e-01,  2.70095348e-01,  5.18418625e-02,\n",
       "       -1.26393393e-01,  1.63251832e-01,  7.22523406e-02,  7.23978505e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_cleaned_vec[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what a linear model can do with this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9470498394749599"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk = StratifiedKFold(n_splits=10)\n",
    "np.mean(cross_val_score(LogisticRegression(),\n",
    "                        final_cleaned_vec,\n",
    "                        yelp_smaller_clean.target,\n",
    "                        cv=sk,\n",
    "                        scoring=\"accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a nonlinear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9262499596292312"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sk = StratifiedKFold(n_splits=3)\n",
    "np.mean(cross_val_score(RandomForestClassifier(n_estimators=50),\n",
    "                        final_cleaned_vec,\n",
    "                        yelp_smaller_clean.target,\n",
    "                        cv=sk,\n",
    "                        scoring=\"accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets split our data and train a new keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: (16000, 300)\n",
      "X_test size: (4000, 300)\n",
      "y_train size: (16000,)\n",
      "y_test size: (4000,)\n",
      "y-split train:\n",
      " 1    0.75575\n",
      "0    0.24425\n",
      "Name: target, dtype: float64\n",
      "y-split test:\n",
      " 1    0.7455\n",
      "0    0.2545\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#using spaCy word vectors\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_cleaned_vec, yelp_smaller_clean.target, test_size=0.2, random_state=21)\n",
    "\n",
    "print('X_train size: {}'.format(X_train.shape))\n",
    "print('X_test size: {}'.format(X_test.shape))\n",
    "print('y_train size: {}'.format(y_train.shape))\n",
    "print('y_test size: {}'.format(y_test.shape))\n",
    "print(\"y-split train:\\n\",y_train.value_counts()/y_train.shape[0])\n",
    "print(\"y-split test:\\n\",y_test.value_counts()/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def build_model(architecture='mlp',input_dim=300):\n",
    "    model = Sequential()\n",
    "    if architecture == 'mlp':\n",
    "        # Densely Connected Neural Network (Multi-Layer Perceptron)\n",
    "        model.add(Dense(512, activation='relu', kernel_initializer='he_normal', input_dim=input_dim))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(512, activation='relu', kernel_initializer='he_normal'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(512, activation='relu', kernel_initializer='he_normal'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(512, activation='relu', kernel_initializer='he_normal'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "    elif architecture == 'cnn':\n",
    "        # 1-D Convolutional Neural Network - you can create a neural network this way as well\n",
    "        inputs = Input(shape=(input_dim,1))\n",
    "\n",
    "        x = Conv1D(64, 3, strides=1, padding='same', kernel_initializer=\"uniform\",activation='relu')(inputs)\n",
    "\n",
    "        #Cuts the size of the output in half, maxing over every 2 inputs\n",
    "        x = MaxPooling1D(pool_size=2)(x)\n",
    "        x = Conv1D(128, 3, strides=1, padding='same', kernel_initializer=\"uniform\",activation='relu')(x)\n",
    "        x = GlobalMaxPooling1D()(x) \n",
    "        outputs = Dense(1, kernel_initializer=\"uniform\", activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='CNN')\n",
    "    else:\n",
    "        print('Error: Model type not found.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text train shape:  (16000, 300, 1)\n",
      "Text test shape:  (4000, 300, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 300, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 300, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 150, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 150, 128)          24704     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 25,089\n",
      "Trainable params: 25,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define keras model\n",
    "# Using MLP in kernel for speed\n",
    "#model = build_model('mlp')\n",
    "model = build_model('cnn')\n",
    "\n",
    "# If the model is a CNN then expand the dimensions of the training data\n",
    "if model.name == \"CNN\":\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    print('Text train shape: ', X_train.shape)\n",
    "    print('Text test shape: ', X_test.shape)\n",
    "    \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.5464 - acc: 0.7558 - val_loss: 0.5215 - val_acc: 0.7455\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4104 - acc: 0.8032 - val_loss: 0.3346 - val_acc: 0.8678\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.2932 - acc: 0.8774 - val_loss: 0.2766 - val_acc: 0.8885\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.2518 - acc: 0.8970 - val_loss: 0.2480 - val_acc: 0.8988\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.2316 - acc: 0.9055 - val_loss: 0.2310 - val_acc: 0.9048\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.2187 - acc: 0.9107 - val_loss: 0.2202 - val_acc: 0.9070\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.2079 - acc: 0.9162 - val_loss: 0.2129 - val_acc: 0.9113\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.2012 - acc: 0.9193 - val_loss: 0.2048 - val_acc: 0.9165\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.1973 - acc: 0.9196 - val_loss: 0.2039 - val_acc: 0.9143\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.1925 - acc: 0.9229 - val_loss: 0.2010 - val_acc: 0.9167\n"
     ]
    }
   ],
   "source": [
    "# Define number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Fit the model to the training data\n",
    "estimator = model.fit(X_train, y_train,\n",
    "                      validation_data=(X_test,y_test),\n",
    "                      epochs=epochs, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot model accuracy over epochs\n",
    "plt.plot(estimator.history['acc'])\n",
    "plt.plot(estimator.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "1. Try the `mlp` model. Does it give similar performance? Is it faster?\n",
    "2. Try to create a neural network model that outperforms the Logistic Regression model we just built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use precomputed spaCy vectors\n",
    "\n",
    "Here, we will attempt to use the vector-space representations of words (word embeddings) that have already been pre-created for us. Since word vectors occur on a per-word basis, and each of our documents is a collection of words, the typical strategy is to generate a word vector representation for each word in the given document (review) and average across all found word vectors. This is also exactly what `spaCy` does by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up text using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean text before feeding it to spaCy\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Define function to cleanup text by removing personal pronouns, stopwords, and punctuation\n",
    "def cleanup_text(docs, logging=True):\n",
    "    proc_docs = []\n",
    "    counter = 1\n",
    "    for doc in nlp.pipe(docs, n_threads=4,disable=['parser', 'ner']):\n",
    "        if counter % 200 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "        counter += 1\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-'] #lemmatize, remove pronouns\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords.words() and tok not in punctuations] #remove stopwords, punctuation\n",
    "        tokens = ' '.join(tokens)\n",
    "        proc_docs.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above does the following:\n",
    "- tokenize on spaces\n",
    "- lowercase every token\n",
    "- lemmatization (find root word for each word in text)\n",
    "- remove stopwords and punctuation\n",
    "\n",
    "This is a fairly involved process and takes some time (this took >10 minutes on my machine). Instead of you having to do this, I've already cleaned the text for you using the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>stars</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I found them through Craigslist. I did the 2 m...</td>\n",
       "      <td>find craigslist 2 maid 68 dollar two hour firs...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great pizza, excellent delivery time &amp; if I ev...</td>\n",
       "      <td>great pizza excellent delivery time ever back ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finding an incredible hairstylist is like find...</td>\n",
       "      <td>find incredible hairstylist like find unicorn ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I attempted to use a coupon for the exact item...</td>\n",
       "      <td>attempt use coupon exact item state picture ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dammit! I had such high hopes for this place. ...</td>\n",
       "      <td>dammit high hope place eat month finally order...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  I found them through Craigslist. I did the 2 m...   \n",
       "1  Great pizza, excellent delivery time & if I ev...   \n",
       "2  Finding an incredible hairstylist is like find...   \n",
       "3  I attempted to use a coupon for the exact item...   \n",
       "4  Dammit! I had such high hopes for this place. ...   \n",
       "\n",
       "                                        cleaned_text  stars  target  \n",
       "0  find craigslist 2 maid 68 dollar two hour firs...      1       0  \n",
       "1  great pizza excellent delivery time ever back ...      5       1  \n",
       "2  find incredible hairstylist like find unicorn ...      5       1  \n",
       "3  attempt use coupon exact item state picture ma...      1       0  \n",
       "4  dammit high hope place eat month finally order...      1       0  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up all text DO NOT RUN THIS AS IT TAKES SOME TIME\n",
    "\n",
    "#yelp_smaller_clean = pd.Series(cleanup_text(yelp.text,logging=True))\n",
    "\n",
    "# LOAD IN THE CLEANED DATA INSTEAD :)\n",
    "yelp_smaller_spacy = pd.read_csv(\"./yelp_smaller_cleaned_spacy.csv\")\n",
    "yelp_smaller_spacy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example original review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We stayed for a week, Buffet was horrible, couldnt get a seat in the pool area at all, steakhouse was great but so expensive and u have to dress nice, couldnt get into other restaurants when needed, and when i made suggestions and a few complaints was just told \"sorry but dont know what to tell you\" and that was the manager. The Avenger station was a joke. I will be going to Vegas again but never staying or playing at Treasure island again.  The only good thing there was the concierge and bell staff and the Coffee Shop staff. Oh and they charge a resort fee on top of the package we paid for months in advance. That added another $400 to my total at check-out!'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_smaller_spacy.values[20][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning with spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stay week buffet horrible could get seat pool area steakhouse great expensive dress nice could get restaurant need make suggestion complaint tell sorry know tell manager avenger station joke go vegas never stay play treasure island good thing concierge bell staff coffee shop staff oh charge resort fee top package pay month advance add another 400 total check'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_smaller_spacy.values[20][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        find craigslist 2 maid 68 dollar two hour firs...\n",
       "1        great pizza excellent delivery time ever back ...\n",
       "2        find incredible hairstylist like find unicorn ...\n",
       "3        attempt use coupon exact item state picture ma...\n",
       "4        dammit high hope place eat month finally order...\n",
       "5        buy izip via rapido urban commuter thoroughly ...\n",
       "6        good pancake ever corned beef hash egg everyth...\n",
       "7        wow let first start say expectation place set ...\n",
       "8        wow look good meal luxury look fresh small fam...\n",
       "9        chill comfortable place laundry owner super ch...\n",
       "10       expect cold fry gross uncarbonated drink cold ...\n",
       "11       one bad nail fill experience ever go pink whit...\n",
       "12       delicious latte pastry make point grab morning...\n",
       "13       live tx life attend even judge bbq cookoff kno...\n",
       "14       absolutely love everything place beer well cra...\n",
       "15       good thing live nowhere near place life expect...\n",
       "16       great experience twice recently upgrade old mo...\n",
       "17       place list almost year finally get chance go f...\n",
       "18       great customer service easy work courteous ser...\n",
       "19       great coffee nice atmosphere work without figh...\n",
       "20       stay week buffet horrible could get seat pool ...\n",
       "21       stay mandalay bay cosmopolitan ceasars aria be...\n",
       "22       really enjoy visit tonight show lot hospitalit...\n",
       "23       hand good pizza service ever pizza amazing fet...\n",
       "24       great food feel like chipotle enchiladas try p...\n",
       "25       something little different usual japanese food...\n",
       "26       arrive wednesday evening find one sight front ...\n",
       "27       ask kate go look really help find exactly look...\n",
       "28       start hmmm feel like bad customer ever see pho...\n",
       "29       great store make travel south carolina little ...\n",
       "                               ...                        \n",
       "19970    take hint see closed sign door- open go valent...\n",
       "19971    surprised low review pei wei frankly like food...\n",
       "19972    reason review even half star short story book ...\n",
       "19973    lunch self- salad bar cheese ball many brazili...\n",
       "19974    go friend 's bachelor party private room along...\n",
       "19975    would give star possible scam rip busines scam...\n",
       "19976    everything single asian girl could ask great p...\n",
       "19977           favourite thing vegas never fail entertain\n",
       "19978    comment direct brad .. read nasty review canin...\n",
       "19979    get anything start waste time ask upcoming app...\n",
       "19980    love midas james big help emergency midas frie...\n",
       "19981    best sephora .. ever dj start .. plenty people...\n",
       "19982    senior picture today get makeup nice keep ask ...\n",
       "19983    horrible experience people call order funeral ...\n",
       "19984    hey whatev ... try stay anywhere else vegas re...\n",
       "19985    terrible fly 5 air canada flight last 2 day ev...\n",
       "19986    tao though usually earlier evening like enough...\n",
       "19987    one edinburgh 's old fine butcher george bowma...\n",
       "19988    j'ai ré classe parmi top restaurant brunch vra...\n",
       "19989    place actually earn 's reputation good pizza p...\n",
       "19990    amazing job 2 sheltie first time groom expect ...\n",
       "19991    recently visit vegas trip one team building ac...\n",
       "19992    husband get wahoo go healthy choice fast food ...\n",
       "19993    yesterday cousins luncheon food good portion g...\n",
       "19994    shop line baby registry notice could save 30.9...\n",
       "19995    recently buy hybrid bike specialized vita lot ...\n",
       "19996    update last review 1/29/2017 management let 10...\n",
       "19997    place new name menu basically raffles close ca...\n",
       "19998    midwife birth first baby lot anxiety question ...\n",
       "19999    favorite ayce sushi spot one time new owner st...\n",
       "Name: cleaned_text, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_smaller_spacy.cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_smaller_spacy.cleaned_text.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "train_vec = []\n",
    "for doc in nlp.pipe(yelp_smaller_spacy.cleaned_text, batch_size=500):\n",
    "    if doc.has_vector:\n",
    "        train_vec.append(doc.vector)\n",
    "    # If doc doesn't have a vector, then fill it with zeros.\n",
    "    else:\n",
    "        train_vec.append(np.zeros((128,), dtype=\"float32\"))\n",
    "        \n",
    "#train_vec = [doc.vector for doc in nlp.pipe(train_cleaned, batch_size=500) if doc.has_vector else np.zeros((128,dtype=\"float32\")]\n",
    "train_vec = np.array(train_vec)\n",
    "\n",
    "end = time()\n",
    "print('Total time passed parsing documents: {} seconds'.format(end - start))\n",
    "print('Total number of documents parsed: {}'.format(len(train_vec)))\n",
    "print('Number of words in first document: ', len(yelp_smaller_spacy[0]))\n",
    "print('Number of words in second document: ', len(yelp_smaller_spacy[1]))\n",
    "print('Size of vector embeddings: ', train_vec.shape[1])\n",
    "print('Shape of vectors embeddings matrix: ', train_vec.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: (16000, 300, 1)\n",
      "X_test size: (4000, 300, 1)\n",
      "y_train size: (16000,)\n",
      "y_test size: (4000,)\n",
      "y-split train:\n",
      " 1    0.75575\n",
      "0    0.24425\n",
      "Name: target, dtype: float64\n",
      "y-split test:\n",
      " 1    0.7455\n",
      "0    0.2545\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#using spaCy word vectors\n",
    "X_train_spacy, X_test_spacy, y_train_spacy, y_test_spacy = train_test_split(train_vec, yelp_smaller_spacy.target, test_size=0.2, random_state=21)\n",
    "\n",
    "print('X_train size: {}'.format(X_train.shape))\n",
    "print('X_test size: {}'.format(X_test.shape))\n",
    "print('y_train size: {}'.format(y_train.shape))\n",
    "print('y_test size: {}'.format(y_test.shape))\n",
    "print(\"y-split train:\\n\",y_train.value_counts()/y_train.shape[0])\n",
    "print(\"y-split test:\\n\",y_test.value_counts()/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, lets build a simple linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8448975940118985"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk = StratifiedKFold(n_splits=10)\n",
    "np.mean(cross_val_score(LogisticRegression(),\n",
    "                        train_vec,\n",
    "                        yelp_smaller_spacy.target,\n",
    "                        cv=sk,\n",
    "                        scoring=\"accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare it to a Keras CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 854,529\n",
      "Trainable params: 854,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define keras model\n",
    "# Using MLP in kernel for speed\n",
    "model_spacy = build_model('mlp',input_dim=128)\n",
    "#model_spacy = build_model('cnn',input_dim=128)\n",
    "\n",
    "# If the model is a CNN then expand the dimensions of the training data\n",
    "if model_spacy.name == \"CNN\":\n",
    "    X_train_spacy = np.expand_dims(X_train_spacy, axis=2)\n",
    "    X_test_spacy = np.expand_dims(X_test_spacy, axis=2)\n",
    "    print('Text train shape: ', X_train_spacy.shape)\n",
    "    print('Text test shape: ', X_test_spacy.shape)\n",
    "    \n",
    "model_spacy.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/30\n",
      "16000/16000 [==============================] - 3s 216us/step - loss: 0.4884 - acc: 0.7873 - val_loss: 0.4209 - val_acc: 0.8317\n",
      "Epoch 2/30\n",
      "16000/16000 [==============================] - 2s 152us/step - loss: 0.4149 - acc: 0.8211 - val_loss: 0.3873 - val_acc: 0.8353\n",
      "Epoch 3/30\n",
      "16000/16000 [==============================] - 2s 153us/step - loss: 0.4047 - acc: 0.8252 - val_loss: 0.3747 - val_acc: 0.8440\n",
      "Epoch 4/30\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.3954 - acc: 0.8298 - val_loss: 0.3837 - val_acc: 0.8420\n",
      "Epoch 5/30\n",
      "16000/16000 [==============================] - 3s 158us/step - loss: 0.3783 - acc: 0.8359 - val_loss: 0.3759 - val_acc: 0.8423\n",
      "Epoch 6/30\n",
      "16000/16000 [==============================] - 2s 156us/step - loss: 0.3785 - acc: 0.8369 - val_loss: 0.3706 - val_acc: 0.8440\n",
      "Epoch 7/30\n",
      "16000/16000 [==============================] - 3s 156us/step - loss: 0.3727 - acc: 0.8379 - val_loss: 0.3683 - val_acc: 0.8455\n",
      "Epoch 8/30\n",
      "16000/16000 [==============================] - 3s 161us/step - loss: 0.3676 - acc: 0.8419 - val_loss: 0.3783 - val_acc: 0.8480\n",
      "Epoch 9/30\n",
      "16000/16000 [==============================] - 2s 153us/step - loss: 0.3633 - acc: 0.8434 - val_loss: 0.3717 - val_acc: 0.8460\n",
      "Epoch 10/30\n",
      "16000/16000 [==============================] - 3s 160us/step - loss: 0.3605 - acc: 0.8421 - val_loss: 0.3660 - val_acc: 0.8490\n",
      "Epoch 11/30\n",
      "16000/16000 [==============================] - 3s 161us/step - loss: 0.3533 - acc: 0.8478 - val_loss: 0.3663 - val_acc: 0.8500\n",
      "Epoch 12/30\n",
      "16000/16000 [==============================] - 2s 156us/step - loss: 0.3468 - acc: 0.8479 - val_loss: 0.3729 - val_acc: 0.8505\n",
      "Epoch 13/30\n",
      "16000/16000 [==============================] - 3s 157us/step - loss: 0.3470 - acc: 0.8512 - val_loss: 0.3655 - val_acc: 0.8528\n",
      "Epoch 14/30\n",
      "16000/16000 [==============================] - 3s 159us/step - loss: 0.3472 - acc: 0.8512 - val_loss: 0.3704 - val_acc: 0.8472\n",
      "Epoch 15/30\n",
      "16000/16000 [==============================] - 3s 158us/step - loss: 0.3422 - acc: 0.8501 - val_loss: 0.3700 - val_acc: 0.8478\n",
      "Epoch 16/30\n",
      "16000/16000 [==============================] - 3s 159us/step - loss: 0.3346 - acc: 0.8559 - val_loss: 0.3951 - val_acc: 0.8293\n",
      "Epoch 17/30\n",
      "16000/16000 [==============================] - 3s 160us/step - loss: 0.3346 - acc: 0.8535 - val_loss: 0.4106 - val_acc: 0.8343\n",
      "Epoch 18/30\n",
      "16000/16000 [==============================] - 3s 185us/step - loss: 0.3259 - acc: 0.8594 - val_loss: 0.3844 - val_acc: 0.8445\n",
      "Epoch 19/30\n",
      "16000/16000 [==============================] - 3s 160us/step - loss: 0.3253 - acc: 0.8587 - val_loss: 0.3815 - val_acc: 0.8485\n",
      "Epoch 20/30\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3219 - acc: 0.8594 - val_loss: 0.4030 - val_acc: 0.8213\n",
      "Epoch 21/30\n",
      "16000/16000 [==============================] - 3s 159us/step - loss: 0.3176 - acc: 0.8644 - val_loss: 0.3929 - val_acc: 0.8498\n",
      "Epoch 22/30\n",
      "16000/16000 [==============================] - 3s 160us/step - loss: 0.3200 - acc: 0.8638 - val_loss: 0.3868 - val_acc: 0.8435\n",
      "Epoch 23/30\n",
      "16000/16000 [==============================] - 3s 158us/step - loss: 0.3115 - acc: 0.8660 - val_loss: 0.3999 - val_acc: 0.8450\n",
      "Epoch 24/30\n",
      "16000/16000 [==============================] - 3s 161us/step - loss: 0.3068 - acc: 0.8674 - val_loss: 0.3947 - val_acc: 0.8423\n",
      "Epoch 25/30\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3045 - acc: 0.8663 - val_loss: 0.4083 - val_acc: 0.8492\n",
      "Epoch 26/30\n",
      "16000/16000 [==============================] - 3s 160us/step - loss: 0.3014 - acc: 0.8705 - val_loss: 0.3977 - val_acc: 0.8430\n",
      "Epoch 27/30\n",
      "16000/16000 [==============================] - 3s 160us/step - loss: 0.3024 - acc: 0.8705 - val_loss: 0.4098 - val_acc: 0.8247\n",
      "Epoch 28/30\n",
      "16000/16000 [==============================] - 3s 158us/step - loss: 0.2969 - acc: 0.8719 - val_loss: 0.4034 - val_acc: 0.8380\n",
      "Epoch 29/30\n",
      "16000/16000 [==============================] - 3s 160us/step - loss: 0.2885 - acc: 0.8776 - val_loss: 0.4146 - val_acc: 0.8403\n",
      "Epoch 30/30\n",
      "16000/16000 [==============================] - 3s 159us/step - loss: 0.2888 - acc: 0.8728 - val_loss: 0.4165 - val_acc: 0.8403\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model_spacy.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Define number of epochs\n",
    "epochs = 30\n",
    "\n",
    "# Fit the model to the training data\n",
    "estimator_spacy = model_spacy.fit(X_train_spacy, y_train_spacy,\n",
    "                      validation_data=(X_test_spacy,y_test_spacy),\n",
    "                      epochs=epochs, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "1. Play around with the parameters for these 2 models as well. Can you get this to get consistent validation accuracies higher than the LR model?\n",
    "2. Why do you think this model doesn't perform as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
